# 过拟合与正则化

https://www.bilibili.com/video/BV164411b7dx?p=39

## 1. 过拟合问题 overfitting

### 线性回归的过拟合

什么是过拟合问题？继续用房价预测的例子作为说明。

如下图最左边，用一次函数显然拟合得并不好，因为住房面积越大，房价越趋于平缓。

这个算法不能很好地拟合训练集，我们把这个问题成为欠拟合(underfitting)，或者这个算法具有高偏差(high bias)。

中间的图用一个二次函数对训练集有较好的拟合程度。

而最右边的训练集，用了一个复杂的四次函数，使这条曲线通过了所有的训练集中的点，但是这条曲线很扭曲。所以尽管它对训练集拟合得很好，但我们认为它并不能很好地预测房价。这种情况就是过拟合(overfitting)，或者我们称之为算法具有高方差(high variance)。

![image-20220204170122678](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204170122678.png)

总结来说，过拟合通常在特征过多的时候出现，模型可以非常好地拟合训练集(代价函数接近0)，但是无法泛化(generalize)到新的样本中。

![image-20220204170331538](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204170331538.png)

### 逻辑回归的过拟合

对于逻辑回归来说，是一样的道理。

如下图模型，最左边用一条直线去进行分类，过于简单不准确；中间比较合适；最右边用了非常复杂的高阶函数以确保每一个训练集的点都正确分类，但是它过度扭曲，泛化性差。

![image-20220204171757333](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204171757333.png)

### 识别和解决过拟合问题

刚才我们只说了仅有2个特征变量的情况。事实上，当我们有非常多的特征变量以及很少的训练数据的时候，往往也会出现过拟合问题。

![image-20220204172350704](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204172350704.png)

为了避免过拟合问题，我们可以：

1. 减少选取的特征数量(人工挑选或者通过model selection algorithm选取)；
2. 正则化(regularization)，保留所有的特征变量，但是减少量级或者参数$\theta$的大小。

![image-20220204172715917](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204172715917.png)

## 2. 代价函数和正则化

### 直觉理解正则化

假如我们在房价预测中得到了一个过拟合的模型，如右图。

为了解决过拟合问题，我们在它的代价函数中对高次项进行惩罚，增加参数$\theta$乘以非常大的系数(本例中为1000)。

这样一来我们得到的高次项的系数$\theta$将会非常非常小，那么高次项在模型中几乎可以忽略不计，于是我们得到的依然近似为一个二次函数。

以上就是正则化背后的思想。

![image-20220204175123585](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204175123585.png)

### 正则化

更小的参数$\theta$代表了更平滑的模型曲线，因此过拟合的风险更小。

由于不知道该选择哪些参数，所以对所有参数都进行惩罚和缩小。即在代价函数的最后增加一个正则项去缩小每个参数的值，如下图

![image-20220204183652591](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204183652591.png)

其中系数$\lambda$被称为正则化参数，其作用是在两个目标之间进行取舍和平衡：目标1，公式前半段用于更好地拟合训练数据；目标2，在公式后半段，保持参数$\theta$尽可能的小，避免过拟合。

![image-20220204184118234](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204184118234.png)

如果我们不小心把正则化参数设置得过大，那么所有参数都会接近0，结果是会得到一条直线模型(欠拟合)

![image-20220204184342936](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204184342936.png)

## 3. 线性回归的正则化

在对代价函数进行正则化后，我们的目标仍然是找到一个参数$\theta$使得代价函数最小。

![image-20220204185024022](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204185024022.png)

### 正则化后的梯度下降

总结来说，与正则化前主要有2个不同：

1. 由于正则化不惩罚$\theta_0$，所以在更新的时候$\theta_0$要单独讨论(与未正则化之前一样)
2. 除了$\theta_0$之外的其他参数要带着正则化项一起求偏导，如下图化简后，$\theta_j$的系数是一个比1略小的正数。也就是说$\theta_j$项在每次更新后都会额外缩小一点

![image-20220204191652970](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204191652970.png)

### 正则化后的正规方程

正规方程是使代价函数最小化的另一种计算方法，相关内容在第四章有提到。

总之，在正规方程法求解的过程中，同样是加入一个正则化项如下图，这样就能得到正则化后的解。(具体推导过程没讲)

![image-20220204192251629](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204192251629.png)

## 4. 逻辑回归的正则化

之前我们提到，逻辑回归也有可能过拟合：有过多的高阶项，或者有过多的特征变量。

在逻辑回归的代价函数中，正则化的方法依然是在后面加上一个正则化项进行惩罚，正则化内容依然不包含$\theta_0$.

![image-20220204193220275](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204193220275.png)

### 梯度下降的正则化

与线性回归的场景相同，但是假设函数的意义不同

![image-20220204193448493](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204193448493.png)

### 高级优化方法的正则化

*(octave操作，没看明白，反正就是增加一个正则项)*

![image-20220204193941253](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220204193941253.png)