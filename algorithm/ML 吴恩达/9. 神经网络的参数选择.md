# 神经网络的参数选择

## 1. 代价函数

### 分类问题

假设我们有一组训练数据，我们用L表示神经网络的层数，$s_l$表示每层的unit数目(不包括bias unit)

如果这是一个二分类问题，那么我们应该只有一个输出单元，输出值是一个实数。

如果这是一个多分类问题(k个类别)，那么我们将会有K个输出单元，输出值是一个K维向量

![image-20220207084249641](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207084249641.png)

### 分类问题的代价函数

由于神经网络的单元使用的是逻辑回归的单元，我们先回归一下逻辑回归的代价函数：

![image-20220207084916063](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207084916063.png)

前面是逻辑回归的代价函数，最后一项是正则化项，不包括$\theta_0$。

而对于神经网络来说，由于有K个逻辑回归输出单元，所以此时每个y和h(x)都是K维向量，也就是有K个值；代价函数里面是逻辑回归里的求和项，在这里变成了并且是k从1到K的所有求和项。

![image-20220207091146559](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207091146559.png)

## 2. 反向传播算法

### 前向传播

首先我们用一个数据点来做一下前向传播，与之前一样， 先向量化计算z，再引入sigmoid函数得到激活值a

![image-20220207092206065](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207092206065.png)

### 反向传播的直觉理解

先计算$\delta_j^{(l)}$，代表第l层第j个节点的误差；

我们知道$\alpha_j^{(l)}$代表了第l层第j个节点的激活值；

所以对于每一个输出值来说，有：$\delta_j=\alpha_j-y_j$；

也可以使用向量化的写法：去掉下标j，所有符号都代表K维向量。

这样我们就得到了输出层的误差。

![image-20220207092955625](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207092955625.png)

现在计算前几层的误差

这里公式里用到的有第3层的参数$\Theta$，误第4层的误差$\delta$，以及第三层输入值为z(3)的时候的求导结果，这个求导结果可以用第三层的激活值$a^{(3)}$表示。

![image-20220207093406011](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207093406011.png)

反向传播算法的名字来源于我们从输出层开始计算$\delta$然后逐渐往前。

### 反向传播算法

*(没看懂)*

![image-20220207094431176](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207094431176.png)

## 3. 理解反向传播算法

### 正向传播

以下图为例，假如我们想计算第三层第2个unit，其计算方法是先算出从第2层的$a^{(2)}$乘以权重得到$z^{(3)}$，然后再引入sigmoid函数得到$a^{(3)}$。

其中，乘以权重的具体过程是：

$z_1^{(3)}=\Theta_{10}^{(2)} \times 1 + \Theta_{11}^{(2)} \times a_1^{(2)} + \Theta_{12}^{(2)} \times a_2^{(2)}$

其实就是第二层的三个unit各自乘以不同的权重得到z1(3)。

![image-20220207114720789](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207114720789.png)

### 代价函数

我们假设这个神经网络仅有一个输出单元，并且忽略正则项。

观察代价函数，我们发现中括号里的求和项是第i个样本的代价函数，我们把它记作cost(i)，这个cost(i)扮演了类似方差的角色，我们索性把它考虑成第i个样本输出值与实际值之间的square difference，代表了预测样本值的准确程度。

![image-20220207120514249](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207120514249.png)

### 反向传播算法

一个直观理解：反向传播算法是在计算这些$\delta$项，我们可以把他们想成是第l层第j个单元中得到的activation的“误差”；

在微积分里，$\delta$项其实是代价函数cost(i)关于$z_j^{(l)}$的偏导数

*(还是没听懂)*

![image-20220207121021672](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207121021672.png)

*（理解：反向传播的规则与正向传播很相似，只不过反向传播的是$\delta$，与正向传播相同的时候，其传播的权重还是由$\Theta$决定的。）*

![image-20220207121531382](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207121531382.png)

---

## * 搞懂反向传播算法







---

