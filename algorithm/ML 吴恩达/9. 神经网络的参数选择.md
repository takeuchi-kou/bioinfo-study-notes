# 训练神经网络

## 1. 代价函数

### 分类问题

假设我们有一组训练数据，我们用L表示神经网络的层数，$s_l$表示每层的unit数目(不包括bias unit)

如果这是一个二分类问题，那么我们应该只有一个输出单元，输出值是一个实数。

如果这是一个多分类问题(k个类别)，那么我们将会有K个输出单元，输出值是一个K维向量

![image-20220207084249641](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207084249641.png)

### 分类问题的代价函数

由于神经网络的单元使用的是逻辑回归的单元，我们先回归一下逻辑回归的代价函数：

![image-20220207084916063](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207084916063.png)

前面是逻辑回归的代价函数，最后一项是正则化项，不包括$\theta_0$。

而对于神经网络来说，由于有K个逻辑回归输出单元，所以此时每个y和h(x)都是K维向量，也就是有K个值；代价函数里面是逻辑回归里的求和项，在这里变成了并且是k从1到K的所有求和项。

![image-20220207091146559](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207091146559.png)

## 2. 反向传播算法

### 前向传播

首先我们用一个数据点来做一下前向传播，与之前一样， 先向量化计算z，再引入sigmoid函数得到激活值a

![image-20220207092206065](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207092206065.png)

### 反向传播的直觉理解

先计算$\delta_j^{(l)}$，代表第l层第j个节点的误差；

我们知道$\alpha_j^{(l)}$代表了第l层第j个节点的激活值；

所以对于每一个输出值来说，有：$\delta_j=\alpha_j-y_j$；

也可以使用向量化的写法：去掉下标j，所有符号都代表K维向量。

这样我们就得到了输出层的误差。

![image-20220207092955625](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207092955625.png)

现在计算前几层的误差

这里公式里用到的有第3层的参数$\Theta$，误第4层的误差$\delta$，以及第三层输入值为z(3)的时候的求导结果，这个求导结果可以用第三层的激活值$a^{(3)}$表示。

![image-20220207093406011](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207093406011.png)

反向传播算法的名字来源于我们从输出层开始计算$\delta$然后逐渐往前。

### 反向传播算法

*(没看懂)*

![image-20220207094431176](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207094431176.png)

## 3. 理解反向传播算法

### 正向传播

以下图为例，假如我们想计算第三层第2个unit，其计算方法是先算出从第2层的$a^{(2)}$乘以权重得到$z^{(3)}$，然后再引入sigmoid函数得到$a^{(3)}$。

其中，乘以权重的具体过程是：

$z_1^{(3)}=\Theta_{10}^{(2)} \times 1 + \Theta_{11}^{(2)} \times a_1^{(2)} + \Theta_{12}^{(2)} \times a_2^{(2)}$

其实就是第二层的三个unit各自乘以不同的权重得到z1(3)。

![image-20220207114720789](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207114720789.png)

### 代价函数

我们假设这个神经网络仅有一个输出单元，并且忽略正则项。

观察代价函数，我们发现中括号里的求和项是第i个样本的代价函数，我们把它记作cost(i)，这个cost(i)扮演了类似方差的角色，我们索性把它考虑成第i个样本输出值与实际值之间的square difference，代表了预测样本值的准确程度。

![image-20220207120514249](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207120514249.png)

### 反向传播算法

一个直观理解：反向传播算法是在计算这些$\delta$项，我们可以把他们想成是第l层第j个单元中得到的activation的“误差”；

在微积分里，$\delta$项其实是代价函数cost(i)关于$z_j^{(l)}$的偏导数

*(还是没听懂)*

![image-20220207121021672](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207121021672.png)

*（尝试理解：反向传播的规则与正向传播很相似，只不过反向传播的是$\delta$*

![image-20220207121531382](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220207121531382.png)



---

## * 3B1B的反向传播算法解释

看一下3B1B对反向传播算法的解释，争取搞明白咋回事儿

3B1B的视频：https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.788.recommend_more_video.1

### 正向传播的向量化计算

第一层的激活值体现在矩阵向量乘法计算里为列向量；

第一层输入值$a^{(0)}$对于第二层第一个单元输出值$z_1^{(1)}$的权重体现在矩阵向量乘法里面为矩阵的第一行，它与列向量相乘得到输出列向量的第一个元素，对应第二层的第一个单元；

对于第二层第二个电源输出值$z_2^{(1)}$的权重体现在乘法里为矩阵的第二行，它与列向量相乘得到输出列向量的第二个元素，对应第二层的第二个单元；

以此类推

![image-20220208082628655](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220208082628655.png)

乘法计算后加上bias unit，再对整个向量取sigmoid，最终得到以下形式，这就是前向传播的向量化计算。

![image-20220208085203469](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220208085203469.png)

另外，现在的神经网络大多使用ReLU训练，而非sigmoid。

### 反向传播和代价函数的负梯度

为了保证以最快速度求出最小值 ，我们按照梯度下降法，求出代价函数的负梯度，以使代价函数以最快速率下降。

负梯度是一个以所有权重和bias为参数的向量，告诉我们如何改变所有连线上的weight和bias才能使代价函数下降的最快。

![image-20220208121121131](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220208121121131.png)

由于梯度向量实在太大(包含所有权重和bias)，我们可以从另一个角度理解：梯度向量的每一项的大小都是在告诉大家代价函数对于每个参数的敏感程度，也就是调整哪个参数（权重）能够对代价函数造成最大的影响。

### 单个训练样本对调整权重的影响

我们假设神经网络还没有训练好，其输出结果看起来很随机，其输出层的激活值并不集中在某一个数字上面。我们要做的是改变weight和bias，以求改变最终的输出层结果。

假设我们手上有一张2的手写图形，我们希望神经网络最终的分类结果是2；

这也就是说，我们希望输出层里，【2】的激活值变得更加接近1，其他激活值变得更加接近0，并且变动的大小与现在值和目标值之间的差异成正比(例如在图中，增加2的激活值比降低8的激活值更加重要，因为后者已经接近目标)

![image-20220208130508986](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220208130508986.png)

- 增大目标分类的激活值

现在我们集中讨论如何让输出层中【2】的激活值变大；

我们知道2的激活值是通过前一层的激活值乘以权重再加上bias，最后取sigmoid函数得到的：

![image-20220208130811630](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220208130811630.png)

因此，我们想增加这个激活值，可以：1. 增大bias b；2.增加权重w；3. 改变上一层的激活值a ；我们看看如何调整权重w；

我们知道与前一层较大激活值a相乘的权重w会造成比较大的影响，所以增加这些与较大激活值相乘的权重的“性价比”最高，所以这些权重值就是我们要调整的目标；再看看如何调整前一层的激活值a；

同样，调整与高权重相乘的激活值，对于增加【2】的激活值的性价比是最高的；然而我们并不能直接调整激活值，而只能依靠调整权重w和bias b来改变激活值。

- 减小非目标分类的激活值

我们除了想让数字【2】的激活值变大以外，还希望让其他数字的激活值减小，与改变2类似，改变其他数字的激活值的话，对调整w和b都有相应的要求；

我们把调整2的要求与调整其他数字的要求全部加起来，就能得到如何改变倒数第二层的总体要求，知道如何改变倒数第二层，就能知道如何改变倒数第三层，以此类推。这就是反向传播的理念。

![image-20220208131725534](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220208131725534.png)

以上是单个训练样本的反向传播过程。我们需要对所有训练样本都进行在这样的计算，去记下如何修改w和b，最后取平均值。(但在实践中我们用minibatch代替全部样本以减小计算量，也就是随机梯度下降stochastic gradient descent)。

这一系列对w和b的微调，就是代价函数的负梯度。

---

## 5.梯度检验

反向传播算法容易出现bug而导致代价函数下降结果很差。为了避免这种情况，我们要进行梯度检验。

### 导数的数值估计

假如我们有一条代价函数曲线，以及曲线上一点$(\theta,J(\theta))$，那么就可以求出这一点的导数以及该导数的数值近似。

如图，该点的导数即切线斜率，可以使用双侧差分进行数值近似：

在曲线上，点$\theta$左移一小步有$(\theta-\epsilon,J(\theta-\epsilon))$，右移一小步有$(\theta+\epsilon,J(\theta+\epsilon))$，这两点之间的斜率就是对点$\theta$导数的数值近似。

$\epsilon$取值要尽量小，但是不要太小，否则在程序里会出现数值问题 ，一般取10-4.

![image-20220209084144649](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209084144649.png)

### $\theta$为向量时偏导数的数值估计

上节讨论的是$\theta$为实数的情况，而当$\theta$为向量时计算过程差不多，只不过变成对每一个维度分别求偏导，最后得到一个梯度向量的近似估计。

如果这个估计与真正的偏导数近似，那我们就可以确信反向传播的实现是正确的。

![image-20220209084936944](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209084936944.png)

### 总结梯度检验步骤

1. 通过反向传播计算DVec；
2. 实现数值梯度检验计算gradApprox；
3. 确保它们数值近似；
4. 在训练数据的时候关闭梯度检验，不要再使用近似数值(因为计算量大，而反向传播更快)

![image-20220209090436545](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209090436545.png)

## 6. 随机初始化

在运行梯度下降算法之前，我们需要为$\theta$选取初始值。

![image-20220209091021916](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209091021916.png)

在逻辑回归中，我们有时会把$\theta$初始化为0，我们看看在神经网络中这样设置是否合适。

### 初始化为0

由于每次的输入值都相等，所以隐藏层的所有值和权重也相等，这意味着所有的unit都在计算相同的特征。

所以这样是行不通的。

![image-20220209093731279](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209093731279.png)

### 随机初始化

刚刚我们遇到的问题被称为“problem of symmetric weights”，所以随机初始化就是解决这种对称问题的方法。

具体来说，对每一个$\theta$值，我们将其初始化为一个范围在$[-\epsilon,\epsilon]$的随机值

![image-20220209102234247](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209102234247.png)

## 7. put it together

###  选择网络架构

在进行训练之前，先选择一种网络架构。

首先我们已经定义了特征集，即输入单元的数量；

其次输出单元的数量由分类问题的类别数目决定；

隐藏层的数目，首先我们考虑用单个隐藏层，这是最常见的；

如果选择使用大于一个隐藏层的话，一般每一个隐藏层的unit数量是相同的，并且隐藏unit越多越好(受到计算量限制)；一般来说，隐藏层的unit数量与输入的特征数量一致或者是特征数量的倍数。

![image-20220209103929487](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209103929487.png)

### 训练神经网络的步骤

1. 构建神经网络，随机初始化权重(很小的随机值，接近0)；
2. 执行前向传播算法：给出输入值，得到输出值h(x)；
3. 计算代价函数$J(\theta)$；
4. 执行反向传播算法，计算偏导数项：用for循环对所有样本执行正向和反向传播，得到激活值a和$\delta$值

![image-20220209104745048](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209104745048.png)

5. 用梯度检查去比较(反向传播算法得到的)偏导数值与的估计数值的近似程度，以保证反向传播方法是正确的。
6. 用梯度下降(或者其他优化方法)和反向传播算法最小化代价函数。（由于此时的代价函数不是凸函数，所以可能得到的是局部最小值）

![image-20220209105526415](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209105526415.png)

![image-20220209105545563](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220209105545563.png)
