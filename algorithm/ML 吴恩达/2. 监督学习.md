# 监督学习

## 1. 线性回归

### 例子 ：房价预测

已知一系列房子尺寸以及对应价格，预测一个新的已知尺寸的房屋价格。

这是一个监督学习问题：因为我们有“正确答案”；

同时这是一个回归问题：我们的输出数据是“真实值”(连续的)；

还有一种监督学习问题叫做分类问题(classification)，它输出的是离散结果，例如根据肿瘤大小预测良性或者是恶性。

![image-20220124102738066](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124102738066.png)

### 训练集的符号表示

在训练集中，我们用一系列符号表示数据：

m：训练样本的数量；

x：输入变量，即特征；

y：输出变量，即目标；

(x, y): 一个训练样本；

$(x^{(i)},y^{(i)})$：第i个训练样本

![image-20220124103451524](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124103451524.png)

### 单变量线性回归（univariant linear regression）

我们用h(hypothesis)表示从x到y的映射，称为假设函数。

在这里，假设函数的一种表现方式为：
$$
h_\theta(x)=\Theta_0 + \Theta_1x
$$
这种假设函数是一个线性回归，且只有一个变量，故称为单变量线性回归。

![image-20220124104849869](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124104849869.png)

于是接下来的问题是，如何选择参数$\theta_0$和$\theta_1$？

## 2. 代价函数

根据$\theta$选择的不同，我们会得到不同的假设函数，例如

![image-20220124105215424](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124105215424.png)

我们选择$\theta$的目标是使得预测值h(x)更好地拟合观察值y，也就是求能够使$(h(x)-y)^2$的平均值最小的$\theta$，用数学公式表达即为下图右上角的公式

![image-20220124112214918](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124112214918.png)

在前面乘以常数1/2是为了让计算更加简单，不改变求出的结果。

我们把这个取最小值的函数定义为代价函数（cost function）。

于是我们的问题变成了，求$\theta_0 \theta_1$使得代价函数取最小值。

这种形式的代价函数被称为平方误差函数(squared error function)，也就是说，还有其他形式的代价函数，但是平方误差函数对于大多数线性回归问题是比较合理的选择。

### 简化代价函数的图像

为了简化计算，我们令$\theta_0=0$，如此一来假设函数就变成了$h_\theta(x)=\theta_1x$，而损失函数$J$就变成了关于$\theta_1$的函数。

![image-20220124150743483](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124150743483.png)

下面我们在特殊训练集(1,1) (2,2) (3,3)中观察假设函数和代价函数的关系。

- $\theta_1=1$

  把$\theta_1=1$带入假设函数和代价函数，得到假设函数如下图左边，体现在代价函数上为点(1,0)

  ![image-20220124151348565](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124151348565.png)

- $\theta_1=0.5$

  同样经过计算，在代价函数上得到点(0.5, 0.58)

  ![image-20220124151642951](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124151642951.png)

- $\theta_1=0$

  代价函数：点(0, 2.3)

  ![image-20220124151748485](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124151748485.png)

通过计算一系列点，我们可以得到代价函数的形状

![image-20220124151847680](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124151847680.png)

而我们的目标，是找到一个$\theta_1$使得代价函数最小，也就是最初的点(1,0)，这个点使得假设函数完美拟合了训练集。这就是找到最小代价函数的意义所在。

### 原代价函数的图像

回到原来的cost function

![image-20220124153532952](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124153532952.png)

在三维情况下，用等高线图表示代价函数

这是一条拟合得不好的线，可以看到等高线图上的点距离底部比较远

![image-20220124154955617](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124154955617.png)

这条线也不太令人满意，但是拟合得比刚才好，可以看到它里底部中心近了一些

![image-20220124155151690](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124155151690.png)

这条线拟合得非常不错，已经接近中心点了

![image-20220124155303152](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124155303152.png)