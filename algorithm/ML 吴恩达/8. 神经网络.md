# 神经网络

https://www.bilibili.com/video/BV164411b7dx?p=43

## 1. 非线性假设

### 我们为什么需要神经网络算法？

假设我们正在处理一个具有很多特征项的分类问题，并且这个分类问题难以用简单的逻辑回归模型去拟合，我们只能用一些高阶多项式取建立分类模型。

但是当我们需要用到高阶多项式并且初始特征个数非常多的时候，这意味着我们的特征空间会急剧膨胀。

所以在特征个数很多的时候利用高阶多项式建立非线性分类器是行不通的。

![image-20220205171058858](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205171058858.png)

### 例子：用计算机视觉识别汽车

直观上来说，让计算机识别汽车的过程就是，我们给计算机提供一系列训练集：一些汽车的照片和一些不是汽车的照片，训练过后再给计算机一张新的汽车照片，希望它能识别出这是汽车。

在训练过程中，计算机识别到的是不同像素点的颜色强度。

假如我们在每张照片上取2个像素点，那么每张照片在这两个像素强度组成的坐标轴上就是一个点。把所有照片放进去，或许汽车和非汽车就会分布在坐标系的不同区域(图中-代表非汽车，+代表汽车)。现在我们需要一个非线性假设去尽量分开这两种样本。

但是事实上我们需要照片中所有的像素点去分辨汽车与非汽车，那么我们所拥有的特征的维度个数就是照片像素点的个数。这是我们仅仅使用灰度图像进行区分的情形。如果使用RGB彩图，那么每个点就有三个值，特征维度还要乘以三。这将是一个高得离谱的数字。

![image-20220205172118711](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205172118711.png)

因此我们无法用逻辑回归去拟合特征过多的数据。这种情况下神经网络就有了用处。

## 3. model representation I

### 神经元与信号传导

神经网络算法是模拟大脑与神经元学习和工作原理的一种算法。

神经元的结构：

input：树突；

output：轴突；

神经元从树突接收信号，进行处理后从轴突传送出去

![image-20220205180413788](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205180413788.png)

神经元之间靠动作电位*(和神经递质\*)*传递信号。*(\*生物狗的矜持)*

### 单个神经元：logistic unit

一个逻辑单元包括

传入变量$x:[(x_0,) x_1,x_2,\cdots]$，其中$x_0=1$，被称为bias unit；

假设参数(parameter,有时也称做权重weight) $\theta$；

sigmoid(logistic) activation function： $h_\theta(x)=\frac{1}{1+e-\theta^Tx}$

![image-20220205182424082](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205182424082.png)

### 神经网络 neural network

神经网络就是一堆单个神经元叠加在一起。

神经网络包括：

input layer：图中的第一层， 在这层输入特征x；

hidden layer：图中的第二层，在实际问题中可能不止一层，任何不输入输入层和输出层的都是隐藏层；

output layer：图中的第三层，在这层输出y

![image-20220205184511060](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205184511060.png)

### 神经网络的数学过程

我们用$a_i^{(j)}$表示第j层的第i个activation，也就是sigmoid function的输出结果；用$\Theta^{(j)}$代表从第j层到第j+1层的每个单元的映射参数，是一个矩阵。

于是神经网络的计算过程如下：

![image-20220205185447463](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205185447463.png)

其中$a^{(2)}$代表了中间隐藏层的计算结果，而$h_\Theta(x)$也可以表示为$a^{(3)}$，因为它是第三层唯一的计算结果。

其中$\Theta$是一个映射矩阵，如果第j层有$s_j$个单元，而第j+1层有第$s_{j+1}$个单元，那么$\Theta$就是一个$s_{j+1} \times (s_j +1)$维的矩阵。 

## 4. model representation II

我们把计算公式中$g$函数括号里的部分用z表示，其中z是输入变量x与参数$\theta$的加权线性组合。

事实上，我们观察第二层全部g函数的z的形式，它与与向量乘法非常相似，我们可以用向量化的方式进行计算：

$z^{(2)}=\Theta^{(1)}x=\Theta^{(1)}a^{(1)}$

运用sigmoid函数计算a2：

$a^{(2)}=g(z^{(2)})$

这样就得到了第二层的activation，继续用上面的形式就可以计算出第三层的结果，即最终的输出结果。

![image-20220205201751349](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205201751349.png)

这种向前传递计算的形式被称为forward propagation(前向传播)。

当我们只看神经网络的最后两层的时候，这就是一个普通的逻辑回归，特征项为隐藏层的a1 a2 a3

![image-20220205202103170](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205202103170.png)

与普通逻辑回归不同的是，这些特征值是通过学习得到的函数输入值，即从第一层映射到第二层的函数。也就是说，其输出所使用的特征是通过自己训练得到的而非由我们输入的。我们只需要设定参数即可。

![image-20220205202357930](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205202357930.png)

### 神经网络的架构(architecture)

架构指的是不同的神经元的连接方式，如下图。

![image-20220205202602859](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220205202602859.png)

## 5(6). 例子 

我们希望用一个神经网络算法拟合下图右边这个数据集。

首先把右边数据简化为左边：
$$
y=x_1 XOR x_2 \\
\quad =x_1 XNOR x_2
$$
![image-20220206115535068](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220206115535068.png)

在拟合XOR之前，我们先拟合一个AND运算

### 神经网络拟合AND运算

有$x_1,x_2={0,1}，y=x_1 AND x_2$；

我们向其中添加一个bias unit $x_0$，然后给这些输入变量赋予权重$\theta$：-30,20,20；也就是 $h_\Theta(x)=g(-30+20x_1+20x_2)$；

把$x_1 x_2$的值带入上式以及sigmoid函数，得到右下角的真值表，我们可以看出这个结果与AND运算是一致的，这就是拟合AND运算的神经网络模型。

![image-20220206121205914](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220206121205914.png)

### 神经网络拟合OR运算

与AND相仿，我们只需要改变权重值，即可拟合OR运算。

![image-20220206121510371](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220206121510371.png)

### 神经网络拟合NOT运算

只需要一个输入变量$x_1$以及bias unit，赋予输入变量一个大的负权重，即可得到NOT运算，如下

![image-20220206122121887](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220206122121887.png)

已知NOT，那么求NOT AND NOT就比较容易了(在两个变量前面分别加比较大的负权重)。

### 神经网络拟合 XNOR运算

XNOR其实就是上述三个运算的联合，也就是说，先求出$x_1 AND x_2$的结果$a_1^{(2)}$，以及$(NOTx_1)AND(NOTx_2)$的结果$a_2^{(2)}$，然后再求出$a_1^{(2)} OR a_2^{(2)}$，这就是输出的结果$h_\theta(x)$。

总体来看，这个神经网络包含一个输入层，一个隐藏层，一个输出层，最后得到一个非线性的决策边界。

![image-20220206124237869](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220206124237869.png)

## 7. 多元分类

用神经网络进行多元分类，事实上是一种one vs all的扩展，如下图：

![image-20220206125315256](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220206125315256.png)

最终我们需要得到的y是一个四维向量，每一维决定该图是否为对应的类别，可以想象成四个并排的逻辑分类器；

在理想状态下，我们得到的y中只有一个1，其他都是0.

![image-20220206125504677](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220206125504677.png)

我们希望最后得到的结果是$h(x)与y$相等或近似。