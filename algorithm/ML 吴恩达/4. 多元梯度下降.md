## 1. 多元线性回归

之前我们用一个feature预测outcome的数值，例如房价的例子。现在我们又增加了一些feature，如图一共四个，来共同预测房价。

由于变量增加了，所以我们分别用上标和下标表示训练集中的样本序号和特征的序号。

![image-20220127164144010](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127164144010.png)

之前的简单回归方程不再适用，我们需要增加参数和变量的个数

![image-20220127164426313](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127164426313.png)

### 回归方程的矩阵表达

首先我们加入一个常数变量$x_0=1$。

因为$\theta$从0开始，所以为了让x对齐，我们让x也从0开始。

然后我们把参数和变量都写成向量形式；

再转置参数向量为一个(n+1)x1的矩阵，令这个转置矩阵与参数x构成的向量相乘，就得到了多元线性回归方程，也就是：
$$
h_{\theta}(x)=\theta^Tx
$$


![image-20220127165323939](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127165323939.png)

## 2. 多元梯度下降

由于我们现在并不把参数$\theta$看做是一系列数值，而是把它整体看做是一个列向量；

而同时，损失函数J是参数$\theta$的函数；

因此，我们可以把损失函数看成是向量$\theta$的函数。

![image-20220127170042260](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127170042260.png)

我们之前学习过的梯度下降算法如左图所示：

$\theta_0和\theta_1$梯度下降的更新分别为：$\theta=\theta-学习率\alpha \cdot(损失函数对\theta求偏导)$；

其中$\theta_0和\theta_1$需要同时更新。

![image-20220127171155177](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127171155177.png)

对于多元方程来说，其计算公式其实是一样的，只是参数$\theta$的个数从2个变成了n+1个。

比如说，我们来看看特征值从1变成2的情况：

![image-20220127171540861](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127171540861.png)

我们很快可以发现，$\theta_0和\theta_1$的更新公式没有发生变化，仅仅是增加了$\theta_2$的更新公式，并且形式上仅仅改变了x的下标(因为增加了变量x的个数)

## 3. 多元梯度下降：特征缩放

### feature scaling 

确保所有特征都拥有相似的数量级。

依然以房价为例，

如果我们第一个特征是房间的大小，取值在(0,2000)；第二个特征是房间的数量，取值在(1,5)；

那么我们画出的代价函数的等高线就会成为一个细长的椭圆，如下图：

![image-20220127173345546](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127173345546.png)

在这种形状的代价函数中，收敛速度不仅很慢，还有可能来回波动，最终才能收敛至最小值。

在这种情况下，我们可以进行特征缩放，也就是右边这样除以其取值范围，我们让特征的取值变得相近(0,1)；

这样一来我们得到的等高图就会变得比较圆；

在这种代价函数中，梯度下降算法会下降得快一些。

![image-20220127174719054](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127174719054.png)

### 缩放的范围

只要确保缩放后的变量取值范围近似于±1即可。

![image-20220127175022021](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127175022021.png)

### mean normalization 均值归一化

在除以取值范围前先减去变量的平均值，让其取值平均分布在0左右。

![image-20220127175431268](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127175431268.png)

分母可以是最大值-最小值，也可以是标准差。但是我们仅仅是为了让梯度下降快一些，所以用最大值-最小值就可以了。

### 学习率的选择

首先我们画出$J(\theta)$的值随着迭代次数增加而下降的曲线图；

如果梯度下降算法运作正常的话，那么每次迭代，J的取值都应该下降。

- 这条曲线的一个用处是帮你判断梯度下降算法是否已经收敛。

另外有一种测试方法可以告诉我们代价函数是否已经收敛：

如果损失函数经过一步迭代之后的下降小于一个很小的值$\epsilon$，这个测试就判断函数已经收敛。

然而，想要选择一个合适的$\epsilon$阈值是比较困难的，因此更好的方法还是制作下面这个曲线图。

![image-20220127184618912](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127184618912.png)

- 这条曲线的另一个作用是告诉我们算法是否正常工作

如果这条曲线开始上升或者波动的话，说明我们应该使用一个更小的学习率

![image-20220127184836024](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127184836024.png)

### 总结：

如果学习率太小，则会收敛缓慢；

如果学习率太大，就会不收敛或者收敛缓慢

在实际选择学习率的时候，我们先找到使函数收敛的最小取值，以及最大取值，然后我们取能够使函数收敛的最大值或者比最大值略小一些的值作为学习率

![image-20220127185137604](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127185137604.png)

## 4. 特征选择和多项式回归

### 一个关于特征选择的例子

依然以房价预测为例：

我们已知信息有房子的临街宽度以及深度，我们可以直接把这两个变量作为特征对房子的价格进行预测，这样一来我们可以得到有宽度和深度这两个特征的线性回归模型；

或者，我们可以用看起来更合理的办法：把宽度和深度相乘得到房子的占地面积，用面积作为特征对房价进行预测，这样一来我们可以得到有房子面积这个特征的简单线性回归模型。

![image-20220128172752028](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128172752028.png)

这个例子告诉我们，我们完全可以根据已有变量选择一个更好的模型，而不是直接用变量作为特征

### 多项式回归 polynomial regression

由于线性模型必须满足可加性，所以不能出现多次项。但是我们可以将多次项定义为另一个特征，来符合模型的标准。

以房价预测为例，我们想拟合如图这个曲线，用肉眼观看就可以知道用直线不可能拟合，它更像是一个$x^3$的曲线；

于是我们在线性方程中添加$x^3$，但是把它当做是一个新的变量对待，赋予一个新的参数，这样就可以使用线性回归的性质预测非线性关系。

![image-20220128174014968](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128174014968.png)

另外，如果方程中存在多次项，那么特征缩放将变得更加重要

### 不仅仅是三次函数

除此了三次函数，我们还可以添加平方根作为特征，这看起来是个拟合更好的选择

![image-20220128174955370](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128174955370.png)

## 5. 正规方程

目前为止，我们选择参数$\theta$的方法是梯度下降法，需要经历迭代才能达到全局最小值。而正规方程则能够提供给我们一种不需要迭代的求参数方法

![image-20220128175327326](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128175327326.png)

### 求$\theta$使代价函数最小

首先我们想象有一个非常简单的代价函数$J(\theta)$，其中$\theta$仅仅是常数而非向量。

$J(\theta)$是$\theta$的一个二次函数，那么求最小值只需求导即可。

![image-20220128175701872](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128175701872.png)

但是在实际应用中，$\theta$并不是一个简单的实数，而是一个n+1维的参数向量，那么我们如果想求向量$\theta$的最小值，就需要对它每一维求偏导。

![image-20220128180243809](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128180243809.png)

### 最小化的损失函数的步骤

还是房价预测的例子，假设我们有一个数据集，包含四个样本。

按照之前的方法为特征X构建一个矩阵，再为结果y构建一个向量。

利用一个矩阵和向量的计算公式，我们就可以直接得到参数$\theta$。

![image-20220128181325068](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128181325068.png)

### 步骤详解

假设我们有一个容量为m的样本；回归模型中有n个特征。

- 构建设计矩阵 design matrix

首先，每一个样本都给我们一个n+1维的特征向量；

然后，把每个特征向量转置，作为设计矩阵的其中一行，直到最后一个样本作为矩阵X的最后一行；

于是我们得到了一个m*(n+1)维的矩阵。

- 构建向量y

只需要按顺序把样本的真实结果填入向量即可

![image-20220128181942806](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128181942806.png)

- 带入等式求解

(X的转置乘以X)的逆矩阵再乘以(X的转置)再乘以向量y

![image-20220128182113236](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128182113236.png)

这个公式将带给我们最优的$\theta$值。

并且运用正规方程法，并不需要特征缩放

![image-20220128182444137](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128182444137.png)

### 如何选择梯度下降和正规方程法

- 梯度下降的缺点是：

  需要选择学习率$\alpha$，这意味着我们需要运行许多次去尝试不同的学习速率；

  需要多次迭代；

- 正规方程的缺点：

  当特征的数量n很大的时候，$(X^TX)^{-1}$是一个n*n的矩阵，而计算逆矩阵的复杂度是$O(n^3)$；

  所以如果n很大的话正规方程法会非常慢，这时选择梯度下降更明智。

当n>10000的时候可以开始倾向于梯度下降法。

另外，对于一些复杂的算法， 正规方程并不适用，我们依然需要选择梯度下降法。

但是对于线性回归模型来说，正规方程法是一个更快的方法。

### 不可逆矩阵

有一些矩阵是没有逆矩阵的，它们被称为特异矩阵或者退化矩阵

矩阵不可逆的原因：

1. 包含多余特征（两个特征是一样的）
2. 包含过多的特征（特征多于样本），这时可以删除特征或者可以正则化（regularization）

所以如果矩阵不可逆，我们先看看模型中有无多余特征，以及是否特征过多；如果特征过多又不能删除，就考虑正则化。

![image-20220128184225203](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220128184225203.png)