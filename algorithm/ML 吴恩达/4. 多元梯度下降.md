## 1. 多元线性回归

之前我们用一个feature预测outcome的数值，例如房价的例子。现在我们又增加了一些feature，如图一共四个，来共同预测房价。

由于变量增加了，所以我们分别用上标和下标表示训练集中的样本序号和特征的序号。

![image-20220127164144010](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127164144010.png)

之前的简单回归方程不再适用，我们需要增加参数和变量的个数

![image-20220127164426313](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127164426313.png)

### 回归方程的矩阵表达

首先我们加入一个常数变量$x_0=1$。

因为$\theta$从0开始，所以为了让x对齐，我们让x也从0开始。

然后我们把参数和变量都写成向量形式；

再转置参数向量为一个(n+1)x1的矩阵，令这个转置矩阵与参数x构成的向量相乘，就得到了多元线性回归方程，也就是：
$$
h_{\theta}(x)=\theta^Tx
$$


![image-20220127165323939](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127165323939.png)

## 2. 多元梯度下降

由于我们现在并不把参数$\theta$看做是一系列数值，而是把它整体看做是一个列向量；

而同时，损失函数J是参数$\theta$的函数；

因此，我们可以把损失函数看成是向量$\theta$的函数。

![image-20220127170042260](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127170042260.png)

我们之前学习过的梯度下降算法如左图所示：

$\theta_0和\theta_1$梯度下降的更新分别为：$\theta=\theta-学习率\alpha \cdot(损失函数对\theta求偏导)$；

其中$\theta_0和\theta_1$需要同时更新。

![image-20220127171155177](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127171155177.png)

对于多元方程来说，其计算公式其实是一样的，只是参数$\theta$的个数从2个变成了n+1个。

比如说，我们来看看特征值从1变成2的情况：

![image-20220127171540861](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127171540861.png)

我们很快可以发现，$\theta_0和\theta_1$的更新公式没有发生变化，仅仅是增加了$\theta_2$的更新公式，并且形式上仅仅改变了x的下标(因为增加了变量x的个数)

## 3. 多元梯度下降：特征缩放

### feature scaling 

确保所有特征都拥有相似的数量级。

依然以房价为例，

如果我们第一个特征是房间的大小，取值在(0,2000)；第二个特征是房间的数量，取值在(1,5)；

那么我们画出的代价函数的等高线就会成为一个细长的椭圆，如下图：

![image-20220127173345546](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127173345546.png)

在这种形状的代价函数中，收敛速度不仅很慢，还有可能来回波动，最终才能收敛至最小值。

在这种情况下，我们可以进行特征缩放，也就是右边这样除以其取值范围，我们让特征的取值变得相近(0,1)；

这样一来我们得到的等高图就会变得比较圆；

在这种代价函数中，梯度下降算法会下降得快一些。

![image-20220127174719054](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127174719054.png)

### 缩放的范围

只要确保缩放后的变量取值范围近似于±1即可。

![image-20220127175022021](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127175022021.png)

### mean normalization 均值归一化

在除以取值范围前先减去变量的平均值，让其取值平均分布在0左右。

![image-20220127175431268](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127175431268.png)

分母可以是最大值-最小值，也可以是标准差。但是我们仅仅是为了让梯度下降快一些，所以用最大值-最小值就可以了。

### 学习率的选择

首先我们画出$J(\theta)$的值随着迭代次数增加而下降的曲线图；

如果梯度下降算法运作正常的话，那么每次迭代，J的取值都应该下降。

- 这条曲线的一个用处是帮你判断梯度下降算法是否已经收敛。

另外有一种测试方法可以告诉我们代价函数是否已经收敛：

如果损失函数经过一步迭代之后的下降小于一个很小的值$\epsilon$，这个测试就判断函数已经收敛。

然而，想要选择一个合适的$\epsilon$阈值是比较困难的，因此更好的方法还是制作下面这个曲线图。

![image-20220127184618912](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127184618912.png)

- 这条曲线的另一个作用是告诉我们算法是否正常工作

如果这条曲线开始上升或者波动的话，说明我们应该使用一个更小的学习率

![image-20220127184836024](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127184836024.png)

### 总结：

如果学习率太小，则会收敛缓慢；

如果学习率太大，就会不收敛或者收敛缓慢

在实际选择学习率的时候，我们先找到使函数收敛的最小取值，以及最大取值，然后我们取能够使函数收敛的最大值或者比最大值略小一些的值作为学习率

![image-20220127185137604](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220127185137604.png)