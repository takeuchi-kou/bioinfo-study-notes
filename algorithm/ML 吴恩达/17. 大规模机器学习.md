# 大规模机器学习

## 1. 学习大数据集

### 动机：机器学习和数据

我们知道， 获取一个高性能的机器学习系统的途径可以是采用低偏差(low-bias)的学习算法，并且用大数据进行训练。

从图中例子我们可以看出，只要使用大量数据去训练算法，效果都会非常好。于是我们得出结论：在机器学习中，通常情况下，决定因素往往不是算法是否够好，而是训练数据是否够多。

![image-20220318153843912](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220318153843912.png)

### 大数据的计算问题

在梯度下降中，我们需要计算一个求和项。但是如果m过大(例如m=100,000,000)，那么在梯度下降的每一步中，我们都需要进行非常庞大的运算量.

在运用这些庞大的数据之前，我们可以先利用一小部分数据去检测这个算法。例如先取1000个样本，看看代价函数在训练集和验证集里面的变化趋势，以判断是否需要增加训练的样本数量。

如下图左，我们可以看出这是一个高方差的算法，验证集的代价函数随着样本量增加而降低，此时我们可以继续增加样本数量以继续改善算法；而下图右边则是一个高偏差算法，验证集和训练集的代价函数很快变得几乎相同并且不再改变，此时增加样本数量也无济于事，我们所应该做的是增加特征项或者神经网络的层数。

![image-20220318154922965](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220318154922965.png)

## 2. 随机梯度下降

对于很多机器学习的算法，我们推导算法的方式是提出一个代价函数，或者一个优化目标，然后使用梯度下降这样的算法求代价函数的最小值。

但是当我们的训练集很大时，梯度下降的计算量将会非常极大。而随机梯度下降则改善了这种情况，可以应用于更大的数据集中。

### 线性回归的梯度下降

首先回顾一下线性回归的梯度下降：

假设函数：向量化乘法；

代价函数：假设函数与目标函数的差的平方的平均数除以2；

以及重复的梯度下降的步骤：代价函数减去学习率乘以导数

从下图我们看到线性回归的代价函数是一个碗状，存在唯一极小值为全局最小值。

![image-20220318155918714](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220318155918714.png)

于是，从等高线图我们可以看到梯度下降有一条轨迹，最后得到参数能够使代价函数取得最小值。

但是，当m很大时，计算梯度下降里的这个微分项将会耗费巨大的计算量，因为要对m个项求和。这种计算方法叫做批量梯度下降(batch gradient descent).

批量梯度下降每一步都要读入全部的数据，而这个新算法每次迭代只需要读入一个数据即可。

![image-20220318160620879](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220318160620879.png)

### 随机梯度下降算法

为了表示简便，我们把目标函数写成:

$J_{train}(\theta)=\frac{1}{m}\sum cost(\theta,(x^{(i)},y^{(i)}))$​

cost表示的是假设函数与真实值之间的差值平方的1/2。

随机梯度下降的首先把所有数据随机打乱顺序，然后用梯度下降法每次拟合一个数据，依次遍历数据集内所有数据。

先打乱数据的顺序可以让我们在遍历训练集的时候，是随机访问训练样本的，它能够让随机梯度下降算法收敛得更快。

与批量梯度下降不同的是，随机梯度下降不需要对全部m个样本求和来得到梯度项，而是只需要对单个训练样本求梯度项

![image-20220318162657791](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220318162657791.png)

随机梯度下降在等高线中的下降轨迹总体上是向全局最小值靠近的，但是路线比较迂回，其收敛形式也并非是在最小值不动，而是在某一个区域徘徊。

另外，当m非常大的时候，外层循环可能仅需要一次就可以得到比较好的结果；但是通常来说，取决于训练集的大小，循环1-10次都是合理的。

![image-20220318163252263](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220318163252263.png)

## 3. mini-batch梯度下降

mini-batch梯度下降是介于batch和随机梯度下降之间的一种方法，它在每次迭代使用b个数据。

通常会选择的b值为b=10，范围在2-100之间都比较常见。

![image-20220318170341253](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220318170341253.png)

mini-batch就是每次迭代对10个数据进行梯度求和，同时循环的步长为10。

mini-batch法比随机梯度下降法有时运行得更好的原因是在于向量化。你可以通过合适的向量化方式，可以在10个样本中实现部分并行计算。

mini-batch方法的缺点在于有一个额外的参数b，我们需要确定b的取值，可能会花费一些时间。

![image-20220318170910454](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220318170910454.png)