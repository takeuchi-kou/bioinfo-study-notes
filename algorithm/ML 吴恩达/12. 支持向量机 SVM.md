# 支持向量机 SVM

## 1. 优化目标

### 复习逻辑回归

如图，右边曲线为sigmoid函数。

如果我们想让某一数据集在y=1的时候，预测值$h_\theta(x)\approx1$，(分类准确并且可信度高)那么该点在图中就表现为z取值非常大，也就是$\theta^T x>>0$;

类似的，在y=0的时候，我们希望预测值$h_\theta(x)\approx0$，该点在图中表现为z取值非常小，也就是$\theta^Tx <<0$

逻辑回归的代价函数为如下形式：

![image-20220215095254885](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215095254885.png)

*(构造这个形式代价函数的原因是要让y=0和y=1能够分情况讨论，详情见第六章逻辑回顾笔记)*

但是由于sigmoid函数的计算量比较大，所以我们将它变为两段直线。

变换后的函数与sigmoid函数功能相似，但是它能够让计算量变小，以及让之后SVM的优化变得简单。

![image-20220215100918661](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215100918661.png)

### support vector machine

在把sigmoid函数换掉的同时，去掉前面的1/m项，再把正则化项的参数$\lambda$去掉，换成前面的代价函数项乘以一个参数C。

以上所有的变换都不会影响最终优化得到的参数(因为改变的都是常数)，经过这样的变换，我们就得到了svm的优化目标函数。

![image-20220215102617822](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215102617822.png)

### SVM的假设函数

与逻辑回归不同的是，svm在得到了优化后的参数$\theta$之后，所输出的$h_\theta(x)$并不是一个概率值，而是根据$\theta^Tx$的值直接输出1或者0.

![image-20220215102816386](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215102816386.png)

## 2. Large margin 的直观理解

有时人们会把SVM称作是大间距分类器large margin classifiers。

如下图，为SVM的代价函数在y=1和y=0时的图像。

当y=1的时候，我们希望z ≥ 1，也就是$\theta^T ≥1$(而不是大于等于0)，这样才可以让损失函数的值=0(而不是小于0.5)；

同理，当y=0的时候，我们希望z ≤ -1，也就是$\theta^T ≤-1$.

如此一来，我们为SVM构建了一个安全间距，保证分类上的准确性。

![image-20220215164020940](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215164020940.png)

### SVM的决策边界

假设我们把常数C设为一个非常大的值，那么在整个代价函数中，我们将会非常迫切地寻找到一个能够使第一项=0的参数$\theta$。

具体来讲，在y=1的时候，由于代价函数是下降的直线，我们需要$\theta^T ≥1$，这样才能使代价函数为0；同理在y=0的时候，我们需要$\theta^T ≤-1$。

![image-20220215164756212](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215164756212.png)

现在我们的优化问题就变成了找到一个参数$\theta$使得第一项等于0，其限制条件就是：

![image-20220215165010117](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215165010117.png)

如果我们把决策边界可视化如下，那么SVM得到的决策边界将会是下图黑色那条看起来非常robust的边界。这条边界与两个类别数据的最小距离都要更大一些。这个距离就被称为支持向量机的间距(margin of the svm)，它使得SVM具有鲁棒性，因为在分离数据的时候，它会使用尽量大的间距去分离。这就是SVM有时候被称为大间距分类器的原因。

![image-20220215165539106](C:\Users\Pc\AppData\Roaming\Typora\typora-user-images\image-20220215165539106.png)

### 当存在 outlier 的时候

我们以上得出的结论都是在把常数C设得非常大的情况下得到的。这样的话，分类器对于离群值会非常敏感。例如下图，可能会因为一个离群值而从黑线变成红线。此时我们可以把C设得小一点，那么边界就会回到黑线那里，这样，就算数据不是线性可分的，SVM也可以进行不错的分类。

![image-20220215170306978](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215170306978.png)

## 3. (可选)大间隔分类器的数学原理

### 两个向量的內积

假设有两个向量u和v，他们两个的內积有两种计算方法：

1. 几何方法：u在v上的投影长度，乘以v本身的长度；
2. 代数运算：$u^Tv=u_1v_1+u_2v_2$

另外，当u和v的夹角大于90度的时候，该值为负数。

![image-20220215171221418](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215171221418.png)

### 把优化函数写为向量形式

为了简化计算，我们假设$\theta_0=0$，并且只有2个特征，即n=2.

于是，根据向量內积，有：

$\theta^Tx=||\theta||\cdot p^{(i)}= \theta_1x_1 + \theta_2x_2$

![image-20220215201952865](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215201952865.png)

将上面的变形带入优化目标函数后，来比较决策边界的两种情况。

首先我们已经把代价函数化简为$\frac{1}{2}||\theta||^2$的形式；

我们由代数运算可以知道参数$\theta$的向量决策边界是垂直的；

如果是下图左边的形式，两边的数据距离决策边界都很近，因为有$\theta^Tx=||\theta||\cdot p^{(i)}$，在p很小的情况下，$\theta$需要很大才可以，这样对于代价函数的优化非常不利；

相反，如果是右图的形式，同理p会稍微增大，这样$\theta$就会降低，有利于优化代价函数；

这就是SVM选择右边这种决策边界的原因，也是其产生大间距分类现象的原因

![image-20220215210027888](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215210027888.png)