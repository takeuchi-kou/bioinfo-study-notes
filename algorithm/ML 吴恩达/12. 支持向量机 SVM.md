# 支持向量机 SVM

## 1. 优化目标

### 复习逻辑回归

如图，右边曲线为sigmoid函数。

如果我们想让某一数据集在y=1的时候，预测值$h_\theta(x)\approx1$，(分类准确并且可信度高)那么该点在图中就表现为z取值非常大，也就是$\theta^T x>>0$;

类似的，在y=0的时候，我们希望预测值$h_\theta(x)\approx0$，该点在图中表现为z取值非常小，也就是$\theta^Tx <<0$

逻辑回归的代价函数为如下形式：

![image-20220215095254885](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215095254885.png)

*(构造这个形式代价函数的原因是要让y=0和y=1能够分情况讨论，详情见第六章逻辑回顾笔记)*

但是由于sigmoid函数的计算量比较大，所以我们将它变为两段直线。

变换后的函数与sigmoid函数功能相似，但是它能够让计算量变小，以及让之后SVM的优化变得简单。

![image-20220215100918661](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215100918661.png)

### support vector machine

在把sigmoid函数换掉的同时，去掉前面的1/m项，再把正则化项的参数$\lambda$去掉，换成前面的代价函数项乘以一个参数C。

以上所有的变换都不会影响最终优化得到的参数(因为改变的都是常数)，经过这样的变换，我们就得到了svm的优化目标函数。

![image-20220215102617822](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215102617822.png)

### SVM的假设函数

与逻辑回归不同的是，svm在得到了优化后的参数$\theta$之后，所输出的$h_\theta(x)$并不是一个概率值，而是根据$\theta^Tx$的值直接输出1或者0.

![image-20220215102816386](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215102816386.png)

## 2. Large margin 的直观理解

有时人们会把SVM称作是大间距分类器large margin classifiers。

如下图，为SVM的代价函数在y=1和y=0时的图像。

当y=1的时候，我们希望z ≥ 1，也就是$\theta^T ≥1$(而不是大于等于0)，这样才可以让损失函数的值=0(而不是小于0.5)；

同理，当y=0的时候，我们希望z ≤ -1，也就是$\theta^T ≤-1$.

如此一来，我们为SVM构建了一个安全间距，保证分类上的准确性。

![image-20220215164020940](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215164020940.png)

### SVM的决策边界

假设我们把常数C设为一个非常大的值，那么在整个代价函数中，我们将会非常迫切地寻找到一个能够使第一项=0的参数$\theta$。

具体来讲，在y=1的时候，由于代价函数是下降的直线，我们需要$\theta^T ≥1$，这样才能使代价函数为0；同理在y=0的时候，我们需要$\theta^T ≤-1$。

![image-20220215164756212](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215164756212.png)

现在我们的优化问题就变成了找到一个参数$\theta$使得第一项等于0，其限制条件就是：

![image-20220215165010117](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215165010117.png)

如果我们把决策边界可视化如下，那么SVM得到的决策边界将会是下图黑色那条看起来非常robust的边界。这条边界与两个类别数据的最小距离都要更大一些。这个距离就被称为支持向量机的间距(margin of the svm)，它使得SVM具有鲁棒性，因为在分离数据的时候，它会使用尽量大的间距去分离。这就是SVM有时候被称为大间距分类器的原因。

![image-20220215165539106](C:\Users\Pc\AppData\Roaming\Typora\typora-user-images\image-20220215165539106.png)

### 当存在 outlier 的时候

我们以上得出的结论都是在把常数C设得非常大的情况下得到的。这样的话，分类器对于离群值会非常敏感。例如下图，可能会因为一个离群值而从黑线变成红线。此时我们可以把C设得小一点，那么边界就会回到黑线那里，这样，就算数据不是线性可分的，SVM也可以进行不错的分类。

![image-20220215170306978](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215170306978.png)

## 3. (可选)大间隔分类器的数学原理

### 两个向量的內积

假设有两个向量u和v，他们两个的內积有两种计算方法：

1. 几何方法：u在v上的投影长度，乘以v本身的长度；
2. 代数运算：$u^Tv=u_1v_1+u_2v_2$

另外，当u和v的夹角大于90度的时候，该值为负数。

![image-20220215171221418](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215171221418.png)

### 把优化函数写为向量形式

为了简化计算，我们假设$\theta_0=0$，并且只有2个特征，即n=2.

于是，根据向量內积，有：

$\theta^Tx=||\theta||\cdot p^{(i)}= \theta_1x_1 + \theta_2x_2$

![image-20220215201952865](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215201952865.png)

将上面的变形带入优化目标函数后，来比较决策边界的两种情况。

首先我们已经把代价函数化简为$\frac{1}{2}||\theta||^2$的形式；

我们由代数运算可以知道参数$\theta$的向量决策边界是垂直的；

如果是下图左边的形式，两边的数据距离决策边界都很近，因为有$\theta^Tx=||\theta||\cdot p^{(i)}$，在p很小的情况下，$\theta$需要很大才可以，这样对于代价函数的优化非常不利；

相反，如果是右图的形式，同理p会稍微增大，这样$\theta$就会降低，有利于优化代价函数；

这就是SVM选择右边这种决策边界的原因，也是其产生大间距分类现象的原因

![image-20220215210027888](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220215210027888.png)

## 4. 核函数 I

### 非线性决策边界

假设我们有如下图这么一个数据集，该如何建立模型呢？

其中一个办法是建立高阶多项式；

我们在表示的时候用$f_i$代替具体的高阶项。

那么，对于f来说，是否有其他更好的特征选择？

![image-20220217162153151](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220217162153151.png)

### 高斯核函数

为了简化计算，我们选取$x_0 x_1 x_2$三个特征；

在特征坐标中随机选取三个点$l^{(1)}  l^{(2)}l^{(3)}$;

对于给定的x来说，我们定义$f_1$为x与$l^{(1)}$之间的相似度；

这个相似度的度量函数，就是高斯核函数

![image-20220217164519705](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220217164519705.png)

### 核函数和相似度

对于$f_1$来说：

如果x与l非常相似，那么$f_1$将会近似为1；

如果x与l非离得非常远，那么$f_1$将会近似为0.

同时，对于一个给定的x，将会产生$f_1 f_2 f_3$三个新的特征，因为我们标记了三个landmark就会有三个相似度。

![image-20220217165518351](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220217165518351.png)

### 直观例子

假如我们的第一个标记$l^{(1)}=(3,5)$,那么核函数$f_1$的图像将会是如下这样凸起的椎体，其最高点刚好在(3,5)上面，且此时取值为1；

另外，$\sigma$决定的是椎体的宽度，当它的值越大的时候椎体越宽，f的下降速度也比较慢；值越小的时候椎体越窄，下降速度越快。

![image-20220217170024652](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220217170024652.png)

### 核函数是如何分类的

如下图，假设我们的回归模型中，$\theta_0=0.5, \theta_1=1 , \theta_2 = 1, \theta_3 = 0$，并且等式左边>1时，我们预测y=1。

经过试验和计算可以发现，当我们的数据点靠近$l^{(i)}$的时候，$f_i \approx 1$，其他f都趋近0，又因为模型中的系数是(0.5,1,1,0)，所以带入等式计算后最终我们可以发现，当x点靠近l1和l2的时候，分类器预测y=1，除此之外y=0，决策边界最终围绕着l1和l2形成两个圆融合的形状。详细图像以及计算步骤见下图。

![image-20220217171032184](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220217171032184.png)

## 5. 核函数 II 

### 如何选择标记点landmark

答案是，对于每个样本x，都在与样本相同的位置设置landmark。

如此一来，landmark的数目就与样本数目已知，并且反映了样本集里面每个样本与其他样本之间的距离。

![image-20220217181233272](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220217181233272.png)

### 得到特征向量

对于一个给定的x，我们都可以求出一个f向量，向量里记录了样本集中每一个点到这个x的相似度；

那么对于任意一个训练样本$(x^{(i)},y^{(i)})$，我们都可以求出一个$f^{(i)}$，并且在其中会有一个自身与自身比较的维度$f_i^{(i)}$。

总之，这个$f^{(i)}$就是描述训练样本的特征向量。

![image-20220217182108039](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220217182108039.png)

### 一些细节

核函数的假设：当$\theta^Tf ≥0$时，y=1(其实就是向量化计算)。

核函数的训练：与SVM一样，只不过把x换成f

![image-20220217183840878](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220217183840878.png)

### 参数选择

- 参数C的选择

  我们说过C相当于$1/\lambda$，所以当C很大的时候，相当于正则化参数很小，这样会得到低偏差和高方差；当C很小的时候，倾向于得到高偏差和低方差

- 参数$\sigma$的选择

  之前提到，当$\sigma$大的时候，核函数下降缓慢，这样比较容易得到高偏差和低方差；当$\sigma$小的时候，核函数下降迅速，这样比较倾向于得到高方差和低偏差。

![image-20220217184438914](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220217184438914.png)

## 6. SVM的使用

### 选择参数和kernel

在实际使用SVM的时候，我们并不去自己编写代码，而是使用现成的库。这时候需要我的去做的就仅仅是选择参数C和kernel函数。

在核函数中，我们可以选择"没有kernel"，即线性核函数，f=x。这个选择适用于当特征数量很大，但是样本数量很小，我们希望避免用一个过于复杂的模型对数据进行过拟合的情况。

我们还可以选择之前讨论的高斯核函数。如果选择高斯核函数，那么我们还需要决定$\sigma$的大小

![image-20220218083921975](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220218083921975.png)

需要注意的是，当使用高斯核函数的时候，如果我们的特征之间的量级相差很大的话，就需要事先进行特征缩放，保证每个特征的数值相差不要特别的大。否则某些数值很小的特征就会被掩盖。

![image-20220218084524567](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220218084524567.png)

### 多分类问题

软件包内一般内置多分类功能，或者也可以采用之前提过的one vs all方法

![image-20220218085936115](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220218085936115.png)

### 逻辑回归 or SVM？

n代表特征数量，m代表训练样本数量。

如果n>>m，也就是特征数量远远大于样本数量，那么一般选择逻辑回归或者no kernel的SVM；

如果 n 非常小(1-1000)，而m比较适中(10-10,000)，那么选择高斯核函数的svm；

如果n<<m，我们可以手动创建更多的特征，然后再使用逻辑回归或者no kernel SVM。

之所以把no kernel SVM与逻辑回归放在一起，是因为他们两个是非常相似的算法

对于以上的情况，神经网络可能都会做得比较好，但是训练起来比较慢。

另外，SVM的优化问题一般是凸优化，所以不用担心是否得到全局最优值。