## 1. 分类问题

分类问题往往是结果有2个或者多个答案，例如：是否为垃圾邮件/是否为欺诈网站 /是否为恶性肿瘤。



![image-20220201083858588](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201083858588.png)

 上述例子均为二分类问题，也存在多分类问题。但我们先讨论二分类。

### 不能对分类问题使用线性回归的原因

线性回归有时或许可以拟合得不错：

![image-20220201084926771](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201084926771.png)

但大多数时候拟合得不好，并且对离群值非常敏感

![image-20220201084909297](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201084909297.png)

并且，在二分类问题中，输出结果为0或1；

然而用线性模型拟合的时候，输出结果可能大于1或者小于0。

而我们接下来将要使用的逻辑回归则会让输出结果在0,1之间。

逻辑回归其实就是一种分类算法。

![image-20220201085229100](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201085229100.png)

## 2. 逻辑回归与假设

### sigmoid/logistic函数

为了让输出结果在0,1之间，我们引入一个新函数：sigmoid/ logistic function,
$$
g(z) = \frac{1}{1+e^{-z}} \\
$$
g(z)的取值范围就在(0,1)之间,其图像如下图右边：

![image-20220201093705968](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201093705968.png)

令$z=\theta^Tx$，我们就得到了一个关于x的输出范围在(0,1)之间的函数，那么其实$g(\theta^Tx)$就是我们需要的函数h(x)。最终得到
$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$

### 输出结果和概率

输出结果的解释：根据x的值预测y=1的概率是多少。

如下图，例如根据肿瘤大小$x_1$的值预测是否为恶性，输出结果$h_\theta(x)=0.7$，那么意味着y=1，即恶性肿瘤的概率是0.7.

![image-20220201100509341](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201100509341.png)

用概率语言表示就是，给出已知x时，y=1的条件概率。

又因为我们能够根据公式求出y=1的条件概率， 那么y=0的条件概率就可以用1减去y=1的条件概率得到了。

![image-20220201100634576](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201100634576.png)

## 3. 逻辑回归的决策边界

在一个逻辑回归模型中，我们假设如果h(x)>0.5，那么我们就认为y=1；如果h(x)<0.5，那么y=0；

又因为，我们引入了sigmoid函数g(z)=h(x)，其中$z=\theta^Tx$；

看sigmoid图像我们知道，当z>0时(也就是$\theta^Tx>0$)，g(z)>0.5,也就是h(x)>0.5,即y=1；相反当z<0(也就是$\theta^Tx<0$)时，y=0

![image-20220201163031082](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201163031082.png)

我们把$\theta^Tx$展开为$\theta_0+\theta_1x_1+\theta_2x_2$,当我们选择了参数之后，就可以确定一个不等式，使得y=1或者y=0了。

例如，参数为[-3;1;1]，则有$-3+x_1+x_2≥0$使得y=1；

表现在图像上如下图，不等式形成一个边界，在直线边界左下的部分使y=0，右上部分使y=1.这条 直线被称为**决策边界**。

这条边界对应了$h_\theta(x)=0.5$的点，把整个平面分为两部分。

决策边界是假设函数的一个属性，由参数$\theta$决定，跟数据集无关。

![image-20220201171849348](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201171849348.png)

### 高阶多项式：非线性决策边界

当我们面对一个非线性的数据集的时候，可以在模型中添加高阶多项式去拟合这些数据，例如下图中可以添加x的二次项以拟合这个图像，得到：

$h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_1x_1^2+\theta_2x_2^2)$

假如我们已经确定了参数$\theta$，那么就可以依靠上面的方法确定一个y=1或0的边界，是一个圆形

所以依靠添加更复杂的多项式，我们可以拟合更复杂的图形，得到更复杂的决策边界，而不仅仅是用直线分开正负样本。

![image-20220201171750304](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201171750304.png)

## 4.代价函数

给定了特征和训练集样本，如何拟合参数$\theta$？、

![image-20220201174342153](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201174342153.png)

### 原先的代价函数

我们知道在线性回归模型中代价函数的形式，通过对代价函数梯度下降可以得到最优的参数；

但是如果我们在逻辑回归中继续沿用这个形式的话，会得到一个参数$\theta$的非凸函数（存在多个局部最优值），在这种函数中运用梯度下降法无法保证得到全局最优值。

而导致非凸函数的原因就在于sigmoid的函数是非线性的，它的平方函数所以是非凸的。因此我们需要找到另外一个代价函数的形式。

![image-20220201175103016](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201175103016.png)

### 新的代价函数

形式以及图像如下，我们首先关注y=1的部分。

![image-20220201175703437](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201175703437.png)

- 代价函数的性质：y=1

我们观察h(x)关于y的函数可以发现，如果h(X)和y同时为0，则cost=0；

如果h(x)趋近于0，则cost激增至正无穷。

第二种情况中对应的直觉理解是，当h(x)=0，也就是在我们的预测中，y=1的概率应该是0，但是实际上y=1，那么我们的代价将会很大。这非常合理。

![image-20220201180118825](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201180118825.png)

- 代价函数的性质：y=0

图像与y=1相反，直觉理解是相似的。

![image-20220201180331258](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220201180331258.png)

## 5. 简化代价函数与梯度下降

在前面，我们已经得到了逻辑回归的代价函数。我们把y分为0和1两种情况讨论。但如此一来，当我们对代价函数进行梯度下降处理的时候会有许多麻烦。于是我们需要找到一种方法把代价函数简化成一个公式。这个公式就是下图手写等式。

![image-20220203144853275](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203144853275.png)

将cost带入代价函数，得到以下这个新的代价函数。

得到代价函数以后，我们就可以求出可以使代价函数最小的参数$\theta$的值了。

![image-20220203145327420](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203145327420.png)

使用梯度下降法，需要对$\theta$求偏导

![image-20220203145540019](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203145540019.png)

结果带入，得到梯度下降的更新公式

![image-20220203145610370](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203145610370.png)

虽然这个更新公式看起来与线性回归的一模一样，但是由于此时$h_\theta(x)$发生了变化，所以并不能说两者是一致的。

![image-20220203150051874](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203150051874.png)

接下来就可以用向量化的方法对代价函数进行迭代更新了。

### 特征缩放

特征缩放的方法同样对于逻辑回归也是适用的。

## 6. 高级优化

在梯度下降法中，已知一个代价函数，那么为了迭代更新，我们需要计算$J(\theta)$的偏导数项，如果还需要监测代价函数迭代的收敛性质的话，就还需要计算$J(\theta)$的值。

![image-20220203151522267](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203151522267.png)

除了梯度下降，还存在其他更加高级和复杂的算法例如BFGS等。利用这些高级算法的好处是：

算法内置一个智能的内循环(line search algorithm)，可以自动尝试不同的学习率，并且选择一个好的学习率，因此我们不需要费力去选择；

算法收敛速度比梯度下降快。

对于这些复杂算法，我们不需要理解原理，只需要直接调用软件库即可(我们要做的事情是多尝试几个库，选择最优的那个)

例如，在octave中，调用复杂算法的步骤如下

![image-20220203152318626](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203152318626.png)

在解决实际机器学习问题中，由于这些高级算法速度更快，所以在运行较大的问题中，倾向于在线性回归和逻辑回归中选择高级算法而非梯度下降。

## 7.多类别分类：one verses all

### 多类别分类的例子

将邮件分到不同文件夹；医学诊断；天气预测等等。在这些例子中，y都是取很少的离散值。

![image-20220203155008525](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203155008525.png)

### 多分类问题的图像

左边是二分类问题的图像，右边是多分类问题的图像。

在二分类问题中，我们用一条直线划分.那么多分类问题呢？

![image-20220203155128569](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203155128569.png)

### 一对多

假如我们有三个类别，可以分成三对独立的二元分类问题：

首先创建一个新的训练集，把class 1当成一个类别， 另外两个当成另一个类别，拟合一个分类器$h_\theta^{(1)}(x)$;

然后把class 2 当成一个单独的类别， 另外两个当成另一个类别，拟合一个分类器$h_\theta^{(2)}(x)$；

对class 3进行同样的操作，拟合分类器$h_\theta^{(3)}(x)$.

如此我们就拟合出了3个分类器，这三个分类器的作用是判断一个点属于类别i的概率。

![image-20220203161556546](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203161556546.png)

总结来说，我们对于每个类别i都创建了一个分类器$h_\theta^{(i)}(x)$，它的作用是预测一个点属于类别i的概率。

当得到一个新的x的时候，我们把x的值带入这些分类器，其中得到最高的h的那个分类器所对应的就是这个x所属的类别，其所对应的y值就是这个x的预测值(i)。

![image-20220203161940366](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220203161940366.png)
