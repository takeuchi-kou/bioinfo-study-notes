# 监督学习

## 1. 线性回归

### 例子 ：房价预测

已知一系列房子尺寸以及对应价格，预测一个新的已知尺寸的房屋价格。

这是一个监督学习问题：因为我们有“正确答案”；

同时这是一个回归问题：我们的输出数据是“真实值”(连续的)；

还有一种监督学习问题叫做分类问题(classification)，它输出的是离散结果，例如根据肿瘤大小预测良性或者是恶性。

![image-20220124102738066](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124102738066.png)

### 训练集的符号表示

在训练集中，我们用一系列符号表示数据：

m：训练样本的数量；

x：输入变量，即特征；

y：输出变量，即目标；

(x, y): 一个训练样本；

$(x^{(i)},y^{(i)})$：第i个训练样本

![image-20220124103451524](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124103451524.png)

### 单变量线性回归（univariant linear regression）

我们用h(hypothesis)表示从x到y的映射，称为假设函数。

在这里，假设函数的一种表现方式为：
$$
h_\theta(x)=\Theta_0 + \Theta_1x
$$
这种假设函数是一个线性回归，且只有一个变量，故称为单变量线性回归。

![image-20220124104849869](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124104849869.png)

于是接下来的问题是，如何选择参数$\theta_0$和$\theta_1$？

## 2. 代价函数

根据$\theta$选择的不同，我们会得到不同的假设函数，例如

![image-20220124105215424](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124105215424.png)

我们选择$\theta$的目标是使得预测值h(x)更好地拟合观察值y，也就是求能够使$(h(x)-y)^2$的平均值最小的$\theta$，用数学公式表达即为下图右上角的公式

![image-20220124112214918](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124112214918.png)

在前面乘以常数1/2是为了让计算更加简单，不改变求出的结果。

我们把这个取最小值的函数定义为代价函数（cost function）。

于是我们的问题变成了，求$\theta_0 \theta_1$使得代价函数取最小值。

这种形式的代价函数被称为平方误差函数(squared error function)，也就是说，还有其他形式的代价函数，但是平方误差函数对于大多数线性回归问题是比较合理的选择。

### 简化代价函数的图像

为了简化计算，我们令$\theta_0=0$，如此一来假设函数就变成了$h_\theta(x)=\theta_1x$，而损失函数$J$就变成了关于$\theta_1$的函数。

![image-20220124150743483](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124150743483.png)

下面我们在特殊训练集(1,1) (2,2) (3,3)中观察假设函数和代价函数的关系。

- $\theta_1=1$

  把$\theta_1=1$带入假设函数和代价函数，得到假设函数如下图左边，体现在代价函数上为点(1,0)

  ![image-20220124151348565](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124151348565.png)

- $\theta_1=0.5$

  同样经过计算，在代价函数上得到点(0.5, 0.58)

  ![image-20220124151642951](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124151642951.png)

- $\theta_1=0$

  代价函数：点(0, 2.3)

  ![image-20220124151748485](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124151748485.png)

通过计算一系列点，我们可以得到代价函数的形状

![image-20220124151847680](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124151847680.png)

而我们的目标，是找到一个$\theta_1$使得代价函数最小，也就是最初的点(1,0)，这个点使得假设函数完美拟合了训练集。这就是找到最小代价函数的意义所在。

### 原代价函数的图像

回到原来的cost function

![image-20220124153532952](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124153532952.png)

在三维情况下，用等高线图表示代价函数

这是一条拟合得不好的线，可以看到等高线图上的点距离底部比较远

![image-20220124154955617](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124154955617.png)

这条线也不太令人满意，但是拟合得比刚才好，可以看到它里底部中心近了一些

![image-20220124155151690](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124155151690.png)

这条线拟合得非常不错，已经接近中心点了

![image-20220124155303152](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220124155303152.png)

## 3. 梯度下降 gradient descent：概述

本节我们使用梯度下降法求任意函数J的最小值。

### set up

- 我们有任一含有两个参数的函数$J(\theta_0,\theta_1)$；

- 目标：求使函数$J(\theta_0,\theta_1)$取最小值的$\theta_0 \theta_1$值。

我们给定$\theta_0 \theta_1$以任意初始值，然后我们改变这两个参数使得函数越来越小，直到获得最小值。

![image-20220125190517789](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125190517789.png)

### intuitive pictures

我们开始于任意一点，然后寻找下降最快的方向，直到收敛于一个局部最优。

于是得到以下轨迹：

![image-20220125191343094](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125191343094.png)

然而，当我们开始于完全不同的点时，它可能收敛于一个不同的局部最优点

![image-20220125191522139](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125191522139.png)

### math

`：=`：赋值，把右边的值赋值给左边；与此同时等号 = 代表assertion。

$\alpha$：learning rate学习率，用于控制梯度下降迈出的步长

- 注意：

对于该方程，我们要同时更新$\theta_0 \theta_1$的值，同步更新的方法是分别把$\theta_0 \theta_1$代入上式求出新的值，存储在temp0和temp1中，这样就能同时更新这两个值了。(而不是先更新一个再更新另一个)

![image-20220125193416183](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125193416183.png)

## 4. 梯度下降：学习率和求导

本节说明梯度下降公式中各项的意义。

为了简化理解过程，我们再次假设函数$J$仅具有一个参数$\theta_1$。

![image-20220125193922441](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125193922441.png)

### 求导的意义

首先我们画出$J(\theta_1)$的函数，然后取任意一点(如图所示)开始梯度下降。

由于该点导数为正数，而学习率也是正数，所以在经过梯度下降过程后，这个点是向着左边移动的，也就是逐渐接近最低点

![image-20220125194554590](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125194554590.png)

另一种情况，当我们取的点在最低点左边的时候，此时曲线的导数为负数，那么在经过梯度下降后，这个点是向着右边移动的，同样逐渐接近最低点

![image-20220125194717825](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125194717825.png)

### 学习率的意义

前面说过，学习率代表了梯度下降的步长。

那么如果步长太小，那么下降速度将会很慢；

如果步长太大，那么有可能错过最低点，甚至开始发散

![image-20220125194945770](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125194945770.png)

### 已经处在最低点？

当已经处在局部最低点的时候，由于此时导数为0，所以梯度下降什么也做不了，这就是我们最终得到的结果

![image-20220125195058004](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125195058004.png)

### 步幅逐渐减小

如图所示，在初始点为粉红色的点时，此时切线陡峭。导数很大，那么其步幅相应的也比较大；当我们越接近最低点时，导数变小，步幅也自动减小，所以我们使用固定的学习率即可，不需要额外将学习率降低。

![image-20220125195723705](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125195723705.png)

## 线性回归和梯度下降

我们学习梯度下降最初的原因是，需要找出线性回归里能够使损失函数最小的参数$\theta_0 和 \theta_1$。

![image-20220125201142543](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125201142543.png)

而为了应用梯度下降法，我们需要求出损失函数的偏导。

![image-20220125201612278](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125201612278.png)

在分别对两个参数求偏导后，得到梯度下降的循环计算公式

![image-20220125201743060](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125201743060.png)

### 局部最优？

我们记得，梯度下降容易是函数陷入局部最优。

![image-20220125202136470](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125202136470.png)

但是，线性回归的代价函数总是这种碗状的函数(凸函数)，所以只有一个全局最优解。

![image-20220125202645912](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125202645912.png)

### 下降过程的原函数变化

最初：

![image-20220125203008885](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125203008885.png)

几步之后

![image-20220125203050531](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125203050531.png)

达到最优值

![image-20220125203125969](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125203125969.png)

### “batch” 梯度下降

由于每次下降都go through了整个训练集(由于求偏导)，所以得名。

另外有一些梯度下降法没有关注整个训练集，而是一些子集。所以这个名字被ML人士使用以示区分。

![image-20220125203520820](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220125203520820.png)