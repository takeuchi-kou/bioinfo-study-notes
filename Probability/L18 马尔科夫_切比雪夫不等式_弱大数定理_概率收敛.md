![image-20211219123223768](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219123223768.png)

# Inequality

## 1. Markov inequality

马尔可夫不等式：利用对概率分布的有限信息做出对极端情况发生概率的估计。

更确切一点， 当X为非负且X期望值较小时，X取一个大数的概率是非常低的。

数学公式表达如图：可以清楚地看出，a取值越大， X>a的概率越小。

![image-20211219132216475](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219132216475.png)

证明过程是构造一个Y，Y为X可能取的最小值。计算Y的期望值，则X的期望值肯定大于Y的期望值。

## 2. The Chebyshev inequality

切比雪夫不等式是马尔科夫不等式的一个应用。

如果随机变量X的方差很小，那么X取值不太可能离期望值太远。

下图为切比雪夫不等式及其从马尔可夫不等式推导出的过程：

![image-20211219135004181](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219135004181.png)

## 3. 切比雪夫不等式 & 马尔科夫不等式

例如在指数型分布中，马尔科夫不等式得出的边界是1/a，但其真实边界$e^{-a}$很显然距离1/a较远，所以这个结论并不算优秀。

而利用切比雪夫不等式得出的结论是$1/a^2$，相较之下更加贴近真实值。

而对于一般情况下也是一样，通常切比雪夫不等式所包含的信息比马尔可夫不等式更为丰富，这是因为马尔科夫不等式仅利用了均值，而切比雪夫不等式同时利用了均值和方差。

![image-20211219140228839](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219140228839.png)

# The Weak Law of Large Numbers (WLLN)

## 1. Main concept

i.i.d.: independent and identically distributed独立且同等分布

$M_n$是样本均值，是所有样本的平均数。由于每一个样本都是随机变量，所以样本均值本身也是随机变量。它应该与真实均值$E[X_i]$​相区分，因为每个随机变量的真实均值应该是一个常数。

由于每一个观测都是独立的，所以我们很容易求出样本均值的期望值和方差。

然后利用切比雪夫不等式得到，样本均值与总体期望值的差值随着样本数的增大而无限趋近于0.

![image-20211219155651574](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219155651574.png)。

## 2. Interpreting

可以解释为一个实验的数次独立重复。

indicator $X_i$=0,1  代表事件A发生与否；

因此$E[X_i]=p$.

其中样本平均数又被称为经验频率，实验重复次数越多期望值与样本平均数越接近，也就是说事件发生的频率与概率越接近。此处是用概率解释频率的一个例证。

![image-20211219171030843](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219171030843.png)



## 3. Example: the pollster's problem

我们希望：计算得到的概率值与真实频率的误差≥0.01的可能性小于5%。

注意区分计算得到的概率值的准确性（误差率0.01），与对我们所估计的准确性的信心（误差过大的可能性小于5%）。

剩下的就是利用切比雪夫不等式与已知信息进行求解即可。

最后，其实对于这个模型，还有比切比雪夫更精确的求法，也就是需要的样本数更少的方法。

![image-20211219182436595](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219182436595.png)

## 4. Converge



大数定理可以浓缩成一句话：样本均值收敛于真实均值。

但是首先我们应该定义一下这里的“收敛”。

概率收敛（converge in probability）：一系列随机变量$Y_n$，它们之间不一定互相独立，存在任意一个$\epsilon>0$，在n趋近于无穷大时，使$|Y_n-a|$大于$\epsilon$的概率趋近于0.此时我们说随机变量$Y_n$概率收敛于数值a。

![image-20211219183418614](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219183418614.png)

下面通过比较实数收敛于概率收敛的区别加深对概率收敛的理解：

概率收敛意味着，几乎随机变量$Y_n$​的所有概率质量，当n非常的大的时候，这个概率质量集中在随机变量收敛值（极限值）附近的窄带里面

![image-20211219185017902](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219185017902.png)。

### 概率收敛的几个性质

如图。

注意，$X_n$收敛于a不代表其期望值收敛于a。

![image-20211219185458562](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211219185458562.png)

### 概率收敛的例子1

下图为概率收敛而期望不收敛的一个例子：

有离散随机变量序列$Y_n$，有1-(1/n)的概率值为0,1/n概率值为$n^2$。

随着n趋近无穷大，概率收敛于0；但是期望值(n)却不收敛。

这是因为概率收敛对尾部极小概率的极大值是不敏感的，而期望值对分布的尾部很敏感。

![image-20211220101830156](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211220101830156.png)

### 概率收敛的例子2

这个例子是$Y_n$是否收敛于0。

证明一个随机变量是否收敛，首先需要找到一个可能的收敛值。在本例中为0.

然后写出它与这个收敛值的差别大于$\epsilon$的不等式，证明这个不等式的成立。

![image-20211220111424403](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211220111424403.png)

# Other Inequalities

## 1. Jensen's inequality

在凸函数中，有随机变量X，则其期望值的函数小于等于该函数的期望值。

（凸函数：二阶导数非负）

证明过程可以把期望代入凸函数的泰勒级数。

![image-20211220121421652](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211220121421652.png)

Jensen's inequality在我们希望知道某个函数g(x)的期望值的相关信息的时候会派上用场。比如当$g(X)=x^4$。

![image-20211220122212752](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211220122212752.png)

## 2. Hoeffding's Inequality

太长了没看懂。。不等式是用来是求i.i.d.的$X_i$的加和的边界。推导结果如下。

![image-20211220124212961](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20211220124212961.png)

