![image-20220112044659767](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112044659767.png)

# 1. 多元回归

## 1.1 多元回归的重要概念

![image-20220112044825588](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112044825588.png)

### 多元回归和简单回归

简单回归只有1个predictor X；

多元回归有多个predictor X1 X2 X3

### 多元回归方程



![image-20220112044958744](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112044958744.png)

- 参数解释：

$\hat{Y}$: Y的估计值

B0: 当所有X=0的时候，Y的估计值

Xk：predictor 

Bk：未标准化的回归系数

$Y-\hat{Y}$：残差（预测误差）

k：predictor 的个数

![image-20220112045031913](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112045031913.png)

- R 和R 方

R：多元相关系数

R即为Y的观测值与Y的估计值的相关系数；

$R^2$​：Y能够被模型解释的方差的占比

这是度量不同模型的性能的一种方式

![image-20220112045744739](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112045744739.png)

## 1.2 例子：预测教师薪水

### 回归模型

Y:：我们将要预测的教师薪水水平

X1(对薪水影响最大的因素)：phD毕业后的时间

X2：出版物的数量

X3：性别

![image-20220112045948390](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112045948390.png)

### 描述性统计信息

![image-20220112050401356](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112050401356.png)

对于性别来说，在计算中我们把男性输入为0，女性输入为1

### 回归方程

在R中运行函数，得到如下方程

![image-20220112050925883](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112050925883.png)

详细的系数表格：

![image-20220112051332616](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112051332616.png)

B：未标准化的回归系数；

SE：标准误差(要记住B只是点估计而已，参考上一章的置信区间)

t：回归系数除以标准误差 B/SE

β：标准化的回归系数

p：p值

### 回归方程中系数的实际含义

![image-20220112051726780](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112051726780.png)

- 46,911是回归常数，它是所有X取0的时候的Y预测值。根据题目，他是某人从博士学位刚毕业，没有出版物，并且性别为男性的薪水

![image-20220112052003788](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112052003788.png)

- 502是出版物的回归系数，它是X2的斜率。在多元回归中，它代表了所有年龄段以及男性女性的出版物的平均斜率，它是一个平均数，考虑了其他所有因素，所以有可能会误导人

![image-20220112052012909](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112052012909.png)

- 哪个性别的薪水更多？我们无法回答这个问题。因为它是一个平均考量：如果纳入研究的女性普遍出版物更少或者获得学位时间更短，那么也有可能得出负的相关系数。

![image-20220112052651591](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112052651591.png)

- 根据模型，男女差异有显著性吗？没有。但是如果我们把其他变量去除，则可能出现差异(因为男性phD平均年限更长)

![image-20220112052906915](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112052906915.png)

- 最强的预测因子是什么？时间。这里要看标准化的回归系数，因为未标准化的系数可能与变量各自的scale有关

![image-20220112052945681](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112052945681.png)

## 2. 矩阵代数

![image-20220112053251267](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112053251267.png)

### 什么是矩阵？

一些已知或未知数字组成的矩形

![image-20220112053418817](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112053418817.png)

### 转置 transpose

![image-20220112054345085](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112054345085.png)

### 加减法

![image-20220112054453527](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112054453527.png)

### 矩阵乘法

矩阵乘法在comformable的时候才能实现，即第一个矩阵 的列数等于第二个矩阵的行数

![image-20220112054606475](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112054606475.png)

符号表示，MN都是4x2矩阵：

![image-20220112054649238](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112054649238.png)

![image-20220112054754897](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112054754897.png)

### 例子

有一个10x3矩阵X，我们把行数想象成观测值，列数想象成不同变量

![image-20220112055133580](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112055133580.png)

我们按照计算相关系数r的步骤对矩阵进行处理。

先复习一下如何计算r

先求叉积之和和方差；两者相除得到比值；比值的平方根就是r。

求叉积先求每个变量的平均数。

先对每个变量求和，用矩阵表示如下

![](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112060854170.png)

在对每个和除以观测数量，得到每个变量的平均数(row vector of means)

![image-20220112063626167](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112063626167.png)

为了进行矩阵减法，我们先使用矩阵乘法一行平均数变成10行，得到均值矩阵(matrix of means)

![image-20220112071636170](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112071636170.png)

然后用原矩阵减去均值矩阵，得到每个值的偏差

![image-20220112071900978](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112071900978.png)

用偏差矩阵的转置乘以偏差矩阵，得到SS/SP矩阵

平方和（SS）是对角线；

叉积之和（SP）在对角线以外

![image-20220112072813422](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112072813422.png)

再把他们除以观测值总数10，得到协方差矩阵

![image-20220112073703963](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112073703963.png)

在乘以标准差矩阵，对协方差进行标准化

![image-20220112073737977](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112073737977.png)

继续标准化，进行如下运算，会得到相关性矩阵（这里看不懂了）

![image-20220112074222253](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112074222253.png)

## 重要概念回顾

![image-20220112074517252](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112074517252.png)

# 3. 回归系数的估计

我们估算系数的目标是，是模型能产生最佳预测，也就是残差最小

![image-20220112080009902](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112080009902.png)

残差最小，也就是估计值与真实值的差异最小，这就是通常的最小二乘估计

![image-20220112080425974](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112080425974.png)

我们以矩阵形式对回归方程进行标准化，也就是把回归常数B0去掉。

这样一来：

Y的预测值就是一个列向量 Nx1，N是观测个数或case个数；

B值是一个列向量 k x 1，k是predictor的个数；

X是一个矩阵，有N行观测值和k列predictor

![image-20220112080817613](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112080817613.png)

现在我们要做的就是解方程，得到B值；

求B值，首先要用Y的观测值代替Y的预测值，因为我们不知道预测值是多少；

然后我们conform X 和 B，让他们能够运用矩阵乘法；

最后我们得出Y=X x B

![image-20220112081211455](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112081211455.png)

在得到这个公式后，我们依然需要解B：

首先我们让X成为对称的正方形，为此需要将X乘以自己的转置矩阵；

![image-20220112081720425](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112081720425.png)

因此等式左右两边分别乘以X的转置矩阵；

为了得到B，我们需要左右两边消去X乘以X的转置矩阵；

要做到这点， 我们再左右两边分别乘以该矩阵的逆(逆矩阵乘以矩阵为单位矩阵)

![image-20220112081828766](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112081828766.png)

消去后，得到B的解

![image-20220112082219576](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220112082219576.png)

以上是手工计算B的过程，在R中用lm()函数实现

