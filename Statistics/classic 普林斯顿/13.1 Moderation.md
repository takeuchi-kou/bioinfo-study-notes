![image-20220114035928331](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114035928331.png)

# Moderation

## moderation & mediation

moderator的地位类似于主持人或领导，它对所有其他效应产生影响

![image-20220114040459339](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114040459339.png)

mediator类似调解员，作用是调节某种关系

![image-20220114040441188](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114040441188.png)

## 例子：刻板印象威胁

变量X： 试验组和控制组（是否施加刻板印象威胁）

变量Y：IQ测验得分

变量Z：工作记忆容量WMC

试验的假设是，我们已知施加刻板印象威胁能够对测验得分造成影响，但是我们想知道如果一个人有很高的WMC，是否能够逃脱这种影响。而这就是一种moderation effect。

moderator用于划分刻板印象威胁作用的对象范围：对WMC低的人起作用，对高的人不起作用

### moderator的引入

当变量Z能够对X和Y的关系造成影响时，Z就能够使回归函数变得更好。

![image-20220114041337078](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114041337078.png)

在本例中，如果刻板印象威胁对于WMC高的人与低的人具有不同的影响程度，那么Z的引入就可以使模型更完善。

### 试验类型：随机试验

意思是说，两个分组来自同一总体，然后被随机分组施加变量X，X作为自变量引起不同的因变量Y。

moderator variable Z 强调的是，自变量X对因变量Y的影响根据Z的分布差异而有所不同。

![image-20220114041846168](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114041846168.png)

### 试验类型：相关性研究(更常见)

这种试验类型中，样本并不来自于同一总体，我们只能通过观察来研究变量之间的相关性。

我们先假设X Y之间存在某种相关系；

moderator variable Z强调的是，X 和Y之间的相关性根据Z的分布不同而有所差异。

![image-20220114042115902](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114042115902.png)

注意：在没有引入Z的回归模型中，回归系数B并不能代表在所有Z的条件下XY的关系，因为XY的关系根据Z的分布改变而发生变化。

![image-20220114042649581](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114042649581.png)

## moderation model

### X和Z都是连续的

把X 和Z作为新的预测变量；

在GLM一章中，这就是我们用线性模型预测非线性关系的方法

如果B3是显著的话，那么Z就具有moderation effect

![image-20220114042802624](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114042802624.png)

### X是categorical的，Z是连续的

在本例中，X是施加刻板印象的情况。这里把它分为三个类别：两个威胁条件和一个控制条件。根据我们再上章学习的dummy coding方法，我们可以设两组dummy code去代表三个分类。如下图回归模型第一行所示，称为main effect；第二行为moderate effect。

![image-20220114074822794](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114074822794.png)

## 如何测试 moderation

### X Z 都是连续的

建立两个模型，第一个模型不加入moderate effect，第二个模型加入，然后比较两个模型，看看是否提高了模型的拟合度。如果提高了，那么就有moderate effect。

![image-20220114075540103](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114075540103.png)

### X 是categorical，Y是连续的

与上面过程相似，不过加入了dummy code

![image-20220114090012803](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114090012803.png)

此时我们通过比较$R^2$来度量两个模型的性能。R方的意义是，Y的方差多大程度被模型所解释。此时有一个NHST来声明其显著性。

我们也可以采用连续的方法，看看moderator的加入是否提高了模型的拟合度。

![image-20220114090456241](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114090456241.png)

## 回到刻板印象的案例

试验的进行过程如下：

学生先完成一个WMC测试；

学生被随机选入如下三个组：显式威胁；隐式威胁；控制组；

学生完成IQ测试。

![image-20220114090756481](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114090756481.png)

### Dummy code的选择

让控制组作为参考组是较为合理的选择

![image-20220114091042516](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114091042516.png)

### 输出结果

我们关注的是，IQtest结果是否受到影响，所以主要关注IQ平均值

![image-20220114091256173](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114091256173.png)

另外，再关注一下moderator对outcome的相关性，可以看到在控制条件下，IQ与WM的相关性很弱，而在威胁条件下非常强。

![image-20220114091607831](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114091607831.png)

我们再看模型1的运行结果

可以看到，回归常数为59.78，它代表了一个没有任何工作记忆的人在受到威胁时的平均智商为59；

-45和-46是刻板印象威胁的影响

![image-20220114091916311](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114091916311.png)

再看模型2的运行结果

我们看到WM*D的回归系数分别为0.47和0.32，他代表的含义是从控制到威胁的斜率的变化，他们的p值显示都是显著的

![image-20220114093814179](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114093814179.png)

看模型1与模型2的对比结果

可以看到p值为0.012，表明差异是显著的

![image-20220114093957099](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114093957099.png)

### 输出图像

最上面的线是控制条件，它表示WMC与IQ之间只有很轻微的正相关关系；

另外两条线是威胁条件，他表示工作记忆与IQ之间有非常强的正相关关系。

如果这三条线平行，那么就表明moderation effect不存在。

但此处明显是存在的。

图中还显示了，当WMC非常高的时候，各组的IQ差异很低，只有在第WMC时，差异才非常大。这也与我们之前结论相一致：搞WMC可以抵御威胁。

![image-20220114094323818](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114094323818.png)

# Centering Predictors

居中意味着每个观测值减去平均数，得到的偏差值。

居中的原因有二：概念性的和统计性的。

![image-20220114100951375](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114100951375.png)

## 概念性原因

### 例子

根据母亲的词汇量预测孩子的语言能力；

moderator是孩子的年龄

![image-20220114101944300](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114101944300.png)

### 原因

从概念上说，回归常数B0代表着当所有的predictor (X Z)位0的时候，Y的预测值；

如果X=0或Z=0是无意义或不可能的话，那么B0将会是很难解释的；

相反，当X=0或Z=0的时候为平均数的话，那么B0将会很容易解释。(在中心化后平均值将会是0)

![image-20220114104410178](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114104410178.png)

回归系数B1是假设Z为平均值的时候X的斜率；

没有moderation effect说明B1在Z处于任何分布状态的时候都是一致的。

![image-20220114104606333](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114104606333.png)

相反，如果存在着moderation effect，那么这说明单一的回归系数B1不足以涵盖所有Z的分布状态，也就是说在Z的不同分布状态的时候，B1是不同的。

![image-20220114110832780](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114110832780.png)

### 图像

no moderation, pure additive

X 和Y在所有Z值上完全相同，即XY的斜率不会随Z的分布变化而改变，及时Z存在自己的斜率

![image-20220114111050464](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114111050464.png)

当我们把上图中心化后，可以看到常数改变了，但是斜率不变；

现在整个平面的中心位于中间格子的中点上

![image-20220114112250868](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114112250868.png)

当存在moderation的时候，XY的斜率随着Z的变化而改变

![image-20220114112509335](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114112509335.png)

中心化后，常数改变了，乘积项的系数也没变，但其他项的系数改变了

![image-20220114112652205](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114112652205.png)

但是中心化的一个好处是，即使我们不知道Z，也能通过把Z设为0来获得最佳估计(平均值)

## 统计学解释

当两个变脸极度相关的时候，会引起GLM模型的崩溃(这跟中心化有什么关系？？)

![image-20220114113325281](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114113325281.png)

## 总结

![image-20220114113421049](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114113421049.png)

# 对第一节的例子进行中心化

### 第一节的例子

![image-20220114123729864](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114123729864.png)

### 两个模型

第一个不包含moderation，第二个包含moderation

![image-20220114124220411](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114124220411.png)



## 对连续变量进行中心化

![image-20220114124200656](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114124200656.png)

### 对试验进行模拟

![image-20220114124358593](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114124358593.png)

- dummy coding scheme

![image-20220114124633395](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114124633395.png)

### 输出结果

- 模型一

中心化前

![image-20220114124750247](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114124750247.png)

中心化后

![image-20220114125015641](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114125015641.png)

现在B0是WM的平均值时候的IQ，中心化使得回归常数更有意义；

并且唯一变化的是回归常数

- 模型二

中心化前

![image-20220114125535371](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114125535371.png)

中心化后

![image-20220114125557293](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114125557293.png)

我们再次得到了回归常数的变化，以及除了moderator之外的数值变化。

- 模型比较

未中心化

![image-20220114125831474](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114125831474.png)

中心化

![image-20220114125850830](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220114125850830.png)

我们看到F值和P值在中心化前后未发生改变