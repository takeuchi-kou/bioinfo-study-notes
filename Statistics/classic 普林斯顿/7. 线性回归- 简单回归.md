![image-20220108075101861](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108075101861.png)

# 1. overview

![image-20220108075209368](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108075209368.png)

## 1.1 回归的定义

回归：根据一个变量或多个predictor变量的得分预测另一个变量的outcome或得分。

简单回归：一个predictor；

多元回归：多个predictor，并且尽可能多。

![image-20220108075454762](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108075454762.png)

### 运动后脑震荡的例子

症状得分作为outcome variable(我们要通过其他变量预测症状得分)；

简单回归：通过一个变量预测症状得分；

多元回归：通过2个变量预测

![image-20220108082431924](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108082431924.png)

## 1.2 回归方程

### 简单回归的方程：

![image-20220108082910501](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108082910501.png)

- R和$R^2$

R的含义是predicted scores和observed scores之间的相关性；

在简单回归的时候，r就是皮尔森相关系数，但是在多元回归的时候，就会稍微复杂一些。

$R^2$的含义是，Y的方差中能够被模型解释的部分。

![image-20220108083739179](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108083739179.png)

- IMPACT的例子(脑震荡症状得分)

简单回归公式：

lm (linear model)

![image-20220108084139574](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108084139574.png)

具体图像和数值：

![image-20220108084257411](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108084257411.png)

此处$R^2$是r(皮尔森相关系数)的平方，这代表了冲动控制(predictor)解释了症状得分((outcome))差异的16%

- regression model

回归模型用于对未来行为进行建模或预测，模型即回归方程

![image-20220108084925775](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108084925775.png)

我们的目标是建立更好的模型，以便于我们更精确地预测未来；

使模型更精确的方法有：

加入更多的predictor variables；

以及建立更好的predictor variables。

### 多元回归方程

我们把症状得分作为结果Y；

冲动控制作为predictor X1；

语言能力作为predictor X2

![image-20220108085253310](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108085253310.png)

用R建立模型得到：

![image-20220108085323002](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108085323002.png)

这次$R^2$变为了22%，也就是说22%的差异可被模型解释，证明模型变得更加精确了。

- 多元回归的可视化

我们无法把多元回归的可视化结果放在同一个散点图中，因为有两个X 

![image-20220108091327434](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108091327434.png)

# 2. estimation of coefficient

### 线性回归：减少残差

想得到以下这个简单回归公式的预测值，我们只需要预测其系数和常数项即可。

![image-20220108091639410](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108091639410.png)

对于coefficient的预测目标是，尽量减少残差(常数项)

![image-20220108091843106](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108091843106.png)

这里使用的方法是**[最小二乘估计](ordinary least squares estimation)**。

![image-20220108092003514](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108092003514.png)

最小二乘估计比较直观的解释

![image-20220108093211176](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108093211176.png)

对于一个简单回归的散点图，只有回归线从中间穿过时，才能保证残差之和最小

![image-20220108093447057](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108093447057.png)

### 用韦恩图代表差异之和

用完整的圆分别代表X和Y的sum of squares (也就是每个数据与平均值之间的差异，即变异程度)；

X和Y的两个圆之间的交叉部分代表了XY共同的变异程度；

如果交叉部分为1，则两个变量完全相关；如果交叉部分为0，则两个变量毫无相关性。

![image-20220108100405760](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108100405760.png)

- 回归模型中的残差表示方法

如果把情况换成是回归模型，则交叉部分就是Y能够被X(predictor)解释的部分，而Y剩下的部分就是残差。

![image-20220108100905940](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108100905940.png)

### unstandardized coefficient

在处理未经标准化的X和Y时，我们需要把r乘以Y和X的标准差之比。

这一步的目的是为了scale Y和X 单位，让他们的影响相同

![image-20220108115245996](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108115245996.png)

### standardized coefficient

在处理经过标准化的X和Y时(z-score)，由于标准差都为1，所以回归系数与相关系数相同，用$\beta$表示。

![image-20220108115625530](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108115625530.png)

# 3. 线性回归的基本假设



线性回归的假设与相关系数的假设非常类似：

Y正态分布；X Y具有线性关系；方差齐次

![image-20220108115800871](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108115800871.png)

可靠、有效、采样随机

![image-20220108115945438](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108115945438.png)

- 老例子：

四个关系中的相关系数相同

![image-20220108120110188](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108120110188.png)

回归方程也相同

![image-20220108120156164](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108120156164.png)

- 验证假设：

储存残差，把残差视为由X函数产生的变量。

![image-20220108120337122](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108120337122.png)

这是X越残差产生的散点图。这些图除了左上角的图，都显示了X与残差存在某种关系，并非完全不相关。

![image-20220108120556544](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108120556544.png)

- 总结假设

![image-20220108120835807](https://gitee.com/joy_thestraydog/typora/raw/master/img/image-20220108120835807.png)