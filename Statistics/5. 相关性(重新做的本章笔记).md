![image-20220105081216964](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105081216964.png)

# 1. overview

## 1.1 定义

- correlation：

  一个统计过程，用于测量和描述两个变量之间的关系。

- correlation 的取值在[-1,1]之间：

  +1是完全正相关；0代表无关；-1代表完全负相关

- 对于两个变量X和Y，是correlation的，其中一个变量可以用于预测另一个：

  更详细一点，一个人在X上的得分可以预测他在Y上的得分。

### 例子

工作记忆能力与智力水平(IQ)强相关;

所以如果我们知道一个人的IQ，我们就可以预测他在工作记忆中的表现如何。

![image-20220105083723622](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105083723622.png)

一个用于显示这种强相关关系的散点图：

x轴是IQ，并且是标准化过的z score——因为其平均数为0.

Y轴是N-back task，是某种评价工作记忆的游戏得分

r=0.8，这属于强相关性

![image-20220105083927267](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105083927267.png)

## 1.2 注意事项

1. correlation并不意味着因果关系；

2. 相关性的大小取决于很多因素，包括采样方式、测量方式：

   继续上面那个例子，如果我们不选取能够代表总体的样本，而是从普林斯顿大学抽样，那么结果会是以下这样的

![image-20220105085654842](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105085654842.png)

可以看到相关性非常低。像这样由于限制了变量之一的范围而降低相关性的现象被称为相关衰减(attenuation of correlation)，因为这样做限制了变量的差异。

3. 相关系数是样本统计量，就像均值一样，无法代表总体，除非相关系数是1.0.

## 1.3 correlation的类型

**Pearson product-moment correlation coefficient(r)**：当变量XY都是连续的；

Point bi-serial correlation:当一个变量连续，另一个是二分的(0,1)；

Phi coefficient: 当两个变量都是二分的；

Spearman rank correlation: 两个变量都是ordinal或ranked(排名或序数).

![image-20220105090440250](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105090440250.png)

![image-20220105090201969](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105090201969.png)

# 2. calculation of r

## 2.1 重要概念

- 皮尔森相关系数(r)的计算：从raw data；从z-score；
- Sum of cross product (SP) & Covariance

![image-20220105092830450](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105092830450.png)

## 2.2 r的定义

- 概念定义：X和Y共同的变异程度，与X和Y分别单独变异的程度具有相关性

- 公式定义：(X和Y的协方差)/(X的方差和Y的方差)

### 从raw-data求r

- 快速复习方差相关计算：

  方差=标准差的平方=均方=sum of squares/N

  ![image-20220105124140454](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105124140454.png)

  由于方差无法对应实际意义，而标准差SD可以：平均每个个体与平均数的偏移程度。

![image-20220105124957036](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105124957036.png)

我们可以把方差SS，看做是每个观测与平均数的偏差再乘以自身，然后求所有观测的平均数。

- 新概念：sums of cross products

与方差概念相对，叉积之和则是X与其平均数的偏差，再乘以Y与其平均数的偏差。

![image-20220105134324641](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105134324641.png)

![image-20220105134430783](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105134430783.png)

- 计算r

我们已经求出了方差和叉积之和，r是这两者之比值的平方根。

我们前面说过，相关系数的含义是，XY共同的变异程度/X和Y的各自独立的变异程度：

其中，XY共同变异程度被叉积之和所捕获，而XY各自的变异程度被方差捕获；

因此，r的求解公式为

![image-20220105134817430](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105134817430.png)

记住他们的来源：

![image-20220105134844808](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105134844808.png)

带入r的公式

![image-20220105134939334](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105134939334.png)

### 从Z-score求r

我们先看从Z-score求r的公式(Z-score是把变量标准化后的结果)

![image-20220105135240714](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105135240714.png)

X和Y变量的Z-score和标准差的求法：

![image-20220105140043540](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105140043540.png)

把标准差带入Z-score公式：

![image-20220105140524419](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105140524419.png)

用Z-score重写r求解公式：

![image-20220105140713380](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105140713380.png)

进行一些代数化简，最终得到的r公式形式与raw data一致

![image-20220105140838028](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105140838028.png)

## 2.3 SP & SS，协方差与方差

方差是sum of square除以总数N，它体现的是某一个变量(X或Y)的变异程度；

协方差是sum of products(叉积之和)除以总数N，体现的是XY共同变异程度。

从z-score公式的角度看，相关系数就像是标准化后的协方差。

由于变量都进行了标准化，所以r的取值在-1到1之间。

![image-20220105141345675](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105141345675.png)

## 2.4 描述性统计和推断性统计

如果进行描述性统计，那么除以总数N；

如果进行的是推断性统计，那么除以N-1

![image-20220105141829496](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105141829496.png)

# 3. 相关性的一些假设

## 3.1 总览6个假设

解释r时进行的一些假设，这样假设不必要但是便于解释

- 变量X和Y是正态分布

- 变量X和Y之间为线性关系

- Homoscedasticity

  

![image-20220105143539583](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105143539583.png)

- X和Y的值是可靠的
- X和Y的测量值是 有效的
- 采样时随机且具有代表性的

![image-20220105143745371](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105143745371.png)

## 如何检测是否违反了假设？

### 假设一：X和Y是正态分布。

只需画出直方图，或进行统计学检验即可。

### 假设二： X和Y是线性关系

画出散点图然后观察相关性

### 假设三：Homoscedasticity(方差齐性)

检查散点图

- 残差与方差齐性

在散点图中，一个点与变量的回归线的垂直距离，反映了其个体的预测误差，也被称为“残差”。

方差齐性的意思是，每个个体的残差不应当与其观测值x有所关联，而应该是随机而偶然的。

通过观察散点图，可以很好地鉴别方差齐性

- 例子

这四幅图的方差和相关系数完全相同。

左上角是符合目前假设的散点图：正态、线性、方差齐；

右上角的XY其实更接近于二次关系而非线性；

左下角的离群点弥补了其他点的正误差；

右下角的图中，在一群点里XY其实完全不存在相关关系，但是在回归线上有一个很远的离群点促进了这种相关关系

![image-20220105144640958](https://gitee.com/joy_thestraydog/typora1.0/raw/master/image-20220105144640958.png)

### 剩下三点假设下节讲解